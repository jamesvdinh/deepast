<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-grand_prize" data-theme="dark" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">$700k/$100k/$50k Grand Prize (Dec 31) | Deep Past Challenge</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:type" content="website"><meta data-rh="true" property="og:url" content="https://scrollprize.org"><meta data-rh="true" property="og:title" content="Deep Past Challenge"><meta data-rh="true" property="og:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:image" content="https://scrollprize.org/img/social/opengraph.jpg"><meta data-rh="true" property="twitter:card" content="summary_large_image"><meta data-rh="true" property="twitter:url" content="https://scrollprize.org"><meta data-rh="true" property="twitter:title" content="Deep Past Challenge"><meta data-rh="true" property="twitter:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="twitter:image" content="https://scrollprize.org/img/social/opengraph.jpg"><link data-rh="true" rel="icon" href="/deepast/img/social/favicon.ico"><link data-rh="true" rel="canonical" href="https://jamesvdinh.github.io/deepast/grand_prize"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/grand_prize" hreflang="en"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/grand_prize" hreflang="x-default"><script src="https://cdn.usefathom.com/script.js" data-site="XERDEBQR" defer="defer" data-spa="auto"></script><link rel="stylesheet" href="/deepast/assets/css/styles.818a908c.css">
<script src="/deepast/assets/js/runtime~main.631ee407.js" defer="defer"></script>
<script src="/deepast/assets/js/main.2a1808c8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/deepast/"><div class="navbar__logo"><img src="/deepast/img/social/favicon-64x64.png" alt="Deep Past Challenge Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/deepast/img/social/favicon-64x64.png" alt="Deep Past Challenge Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Past Challenge</b></a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="docItemContainer_Djhp"><article><div class="theme-doc-markdown markdown"><header><h1>$700k/$100k/$50k Grand Prize (Dec 31)</h1></header>
<p>The $700,000 Grand Prize will go to the first team to read four passages of text from the inside of the two intact scrolls. We also have prizes for second place ($100,000) and third place ($50,000). More details on the qualifying criteria are available <a href="#qualifying-criteria">here</a>.</p>
<p>Here are the scrolls in question:</p>
<div class="flex w-[100%]"><div class="w-[100%] mb-2 mr-2"><img src="/img/overview/scroll1-small-actual.webp" class="w-[100%]"></div><div class="w-[100%] mb-2"><img src="/img/overview/scroll2-small-actual.webp" class="w-[100%]"></div></div>
<div class="flex w-[100%]"><div class="w-[39.2%] mb-2 mr-2"><img src="/img/overview/PHerc332.webp" class="w-[100%]"></div><div class="w-[60.8%] mb-2"><img src="/img/overview/PHerc1667.webp" class="w-[100%]"></div></div>
<p>We have provided you with 3D X-ray scans of each of these scrolls, which you can find <a href="/deepast/data">here</a>. Your job is to extract the text from these scans.</p>
<p>You can approach this challenge through any means necessary: machine learning, computer vision, or machine-assisted tools operated by humans.</p>
<figure><video autoplay="" playsinline="" loop="" muted="" class="w-[100%]" poster="/img/overview/scroll-inside-animation-4.webp"><source src="/img/overview/scroll-inside-animation-4.webm" type="video/webm"></video><figcaption class="mt-0">3D X-ray scan of a scroll <a href="https://www.youtube.com/watch?v=PpNq2cFotyY">(source)</a></figcaption></figure>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-this-difficult">Why is this difficult?<a href="#why-is-this-difficult" class="hash-link" aria-label="Direct link to Why is this difficult?" title="Direct link to Why is this difficult?">​</a></h4>
<p>As you will read in the <a href="/deepast/tutorial">tutorials</a>, advanced tools and techniques exist for virtually unwrapping papyrus scrolls. This was demonstrated in 2015 when Dr. Seales&#x27;s team <a href="https://www2.cs.uky.edu/dri/the-scroll-from-en-gedi/" target="_blank" rel="noopener noreferrer">unwrapped the En-Gedi scroll</a>, and in their recent result <a href="https://arxiv.org/abs/2304.02084" target="_blank" rel="noopener noreferrer">identifying ink from 3D X-ray scans in the Herculaneum scrolls</a>.</p>
<div>But the Herculaneum scrolls have proved more challenging. The remaining challenges include:</div>
<ul>
<li>Segmenting the scrolls. The Herculaneum scrolls are especially long, tightly wrapped, damaged, and distorted. To date, no one has successfully done a large-scale segmentation of these scrolls to identify the surfaces of all the rolled layers.</li>
<li>Finding the ink. The ink used in the Herculaneum scrolls is <a href="https://en.wikipedia.org/wiki/Radiodensity" target="_blank" rel="noopener noreferrer">radiolucent</a>, making it difficult to see in the scans. Recently, Dr. Seales&#x27;s team has trained a machine learning model which can detect the ink from subtle patterns in the 3D X-rays. This works in the fragments, but these models are not yet perfect and will probably need to be improved to work at the scale of an entire scroll.</li>
<li>Putting it all together. Applying the ink detection models to the segmented scroll has not yet been successfully demonstrated.</li>
</ul>
<p>Based on the <a href="https://arxiv.org/abs/2304.02084" target="_blank" rel="noopener noreferrer">landmark results</a> that Dr. Seales and his team have recently produced, we believe that it is possible to read the Herculaneum scrolls using the <a href="/deepast/data">scans</a> that we already have and the tools and techniques that they have developed. And that is Vesuvius Challenge!</p>
<p>The Grand Prize deadline is 11:59pm Pacific, December 31st, 2023.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qualifying-criteria">Qualifying criteria<a href="#qualifying-criteria" class="hash-link" aria-label="Direct link to Qualifying criteria" title="Direct link to Qualifying criteria">​</a></h3>
<div>A <a href="#review-process">Review Team</a> made up of technical experts and <a href="https://en.wikipedia.org/wiki/Papyrology">papyrologists</a> will assess all Grand Prize submissions to ensure that they can:</div>
<ul>
<li>Read at least 4 passages from the available full-scroll data, each containing at least 140 characters of contiguous text (e.g. within the same column)</li>
<li>Verify that each passage contains no more than 15% of characters which are missing or illegible</li>
<li>The 140 characters per passage include the 15% of characters which may be missing or illegible, so 119 characters must be legible. Legible characters only count as legible when identified on a letter-by-letter basis without papyrological interpolation.</li>
<li>Confirm that submissions contain legitimate and linguistically plausible text.</li>
<li>Independently reproduce and verify your results using your code and documented techniques.</li>
</ul>
<p>If no team meets the criteria by the deadline, we reserve the right to award the prizes to the teams that came closest. <strong>This is not a guarantee</strong> — we will only award prizes if we believe the spirit of the prize has substantially been met and if a submission comes very close to the objective threshold. This is entirely at our discretion. If you are very very close to meeting the bar, we encourage you to submit your work before the deadline.</p>
<div>Prizes are awarded in the order that qualifying submisisons are made:</div>
<ul>
<li>1st place: $700,000</li>
<li>2nd place: $100,000</li>
<li>3rd place: $50,000</li>
</ul>
<p>To qualify for the Grand Prize, you must have registered at the time you downloaded the data. The registration form can be found on the <a href="/deepast/data">data page</a>.</p>
<p>We will work with the Grand Prize winners to verify their results, put them in a historical context, and co-publish them in academic venues. The winning code will be made public under a permissive open source license, so that others can reproduce and build on your work.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="submitting-your-result">Submitting your result<a href="#submitting-your-result" class="hash-link" aria-label="Direct link to Submitting your result" title="Direct link to Submitting your result">​</a></h3>
<div>If you have a qualifying result, submit it for consideration by sending an email to <a href="mailto:grandprize@scrollprize.org"><a href="mailto:grandprize@scrollprize.org" target="_blank" rel="noopener noreferrer">grandprize@scrollprize.org</a></a> and provide the following:</div>
<ul>
<li><strong>Images.</strong> Submissions must be in the form of images of the virtually unwrapped papyrus, showing visible and legible text.<!-- -->
<ul>
<li>Submit a single static image for each text region. Images must be generated programmatically, as direct outputs of CT data inputs, and should not contain manual annotations of characters or text.</li>
<li>Specify which scroll each image came from.</li>
<li>Specify where in the scroll they were found: include information about the position of the text vertically as well as radially within the scroll. One easy way to do this is to provide images showing the 3D position of the text surface inside the scroll.</li>
<li>Include scale bars showing the size of 1cm on each submission image.</li>
</ul>
</li>
<li><strong>Methodology.</strong> A detailed technical description of how your solution works. We need to be able to reproduce your work, so please make this as easy as possible:<!-- -->
<ul>
<li>For fully automated software, please create a Docker image that we can easily run to reproduce your work, and please include system requirements.</li>
<li>For software with a human in the loop, please provide written instructions and a video explaining how to use your tool. We’ll work with you to learn how to use it, but we’d like to have a strong starting point. It should take us no more than an hour to perform the human tasks to get your software working.</li>
<li>Either attach your code/video directly to the email, or include an easily accessible link from which we can download it.</li>
</ul>
</li>
<li><strong>Hallucination mitigation.</strong>
<ul>
<li>If there is any risk of your model hallucinating results, please let us know how you mitigated that risk. Tell us why you are confident that the results you are getting are real.</li>
<li>We strongly discourage submissions that use window sizes larger than 0.5x0.5 mm to generate images from machine learning models. This corresponds to 64x64 pixels for 8 µm scans. If your submission uses larger window sizes, we may reject it and ask you to modify and resubmit.</li>
</ul>
</li>
<li><strong>Other information.</strong> Feel free to include any other things we should know.</li>
</ul>
<p>If you’re competing as a team, please have your team leader submit your results. We will communicate with the team leader exclusively, and any prize money will be distributed according to the instructions of the team leader. You’d have to sort out within your team how to split any prizes.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="review-process">Review process<a href="#review-process" class="hash-link" aria-label="Direct link to Review process" title="Direct link to Review process">​</a></h3>
<p>All submissions will be assessed by the Review Team, which consists of a technical team to review your methodology, and an independent team of papyrologists to review your results. Note that the technical team also judges the Open Source Prizes.</p>
<div class="font-bold">Technical team</div>
<ul>
<li>Dr. Brent Seales</li>
<li>Nat Friedman</li>
<li>Dr. Stephen Parsons</li>
<li>Seth Parker</li>
<li>JP Posma</li>
<li>Daniel Havíř</li>
</ul>
<div class="font-bold">Papyrology Team</div>
<ul>
<li>Robert Fowler, Fellow of the British Academy; Professor Emeritus of Classics, Bristol University</li>
<li>Tobias Reinhardt, Corpus Christi Professor of the Latin Language and Literature, Oxford</li>
<li>Richard Janko, Professor of Classics, University of Michigan</li>
<li>Federica Nicolardi, Professor of Classics, University of Naples Federico II</li>
<li>Gianluca Del Mastro, Professor of Papyrology, l’Università della Campania «L. Vanvitelli»</li>
<li>Daniel Delattre, Emeritus Research Director and Papyrologist, CNRS and IRHT</li>
</ul>
<p>We will process your submission as follows:</p>
<div><strong>1. Technical assessment.</strong> The technical team will look at your method, and try to reproduce your results independently. We may also try to apply your techniques to the held-back part of the scrolls to see if they are able to generate new results there.</div>
<ul>
<li>We want to make sure that your method doesn’t hallucinate, so please let us know how you mitigated this risk.</li>
<li>We will work with you on reproducing your solution. We might have questions, such as how your code works, how to use your manual tools (if applicable), and so on. Please make it as easy for us to run your code as reasonably possible, but also don’t wait until your solution is perfect. If you have any questions, or if you’re wondering if you’re ready to submit, just reach out!</li>
<li>We will acknowledge having received your submission within a week. Depending on the difficulty of verifying your methodology, it might take longer until we are able to make our final assessment.</li>
<li>In case there are multiple teams that submit qualifying results, the team that submitted first will win (independent of how long our assessment takes).</li>
</ul>
<div><strong>2. Papyrological assessment.</strong> Once we are reasonably confident that your solution is technically valid and appears to meet the qualifications, we will share your results with the papyrology team, who will judge if the text is plausible and legitimate.</div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/12_grand_prize.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Overview</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/deepast/get_started">Getting Started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Past Challenge.</div></div></div></footer></div>
</body>
</html>