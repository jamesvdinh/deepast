<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-28_2024_prizes" data-theme="dark" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Prizes | Deep Past Challenge</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:type" content="website"><meta data-rh="true" property="og:url" content="https://scrollprize.org"><meta data-rh="true" property="og:title" content="Vesuvius Challenge"><meta data-rh="true" property="og:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:image" content="https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"><meta data-rh="true" property="twitter:card" content="summary_large_image"><meta data-rh="true" property="twitter:url" content="https://scrollprize.org"><meta data-rh="true" property="twitter:title" content="Vesuvius Challenge"><meta data-rh="true" property="twitter:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="twitter:image" content="https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"><link data-rh="true" rel="icon" href="/deepast/img/social/favicon.ico"><link data-rh="true" rel="canonical" href="https://jamesvdinh.github.io/deepast/2024_prizes"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/2024_prizes" hreflang="en"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/2024_prizes" hreflang="x-default"><script src="https://cdn.usefathom.com/script.js" data-site="XERDEBQR" defer="defer" data-spa="auto"></script><link rel="stylesheet" href="/deepast/assets/css/styles.3b93ac60.css">
<script src="/deepast/assets/js/runtime~main.a221a016.js" defer="defer"></script>
<script src="/deepast/assets/js/main.bc0da2ca.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/deepast/"><div class="navbar__logo"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Past Challenge</b></a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="docItemContainer_Djhp"><article><div class="theme-doc-markdown markdown"><header><h1>Prizes</h1></header>
<!-- -->
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37">üéâ</span>2024 Submissions Closed</div><div class="admonitionContent_BuS1"><p>Submissions are now CLOSED for 2024 prizes. We are reviewing them and will announce next steps soon!</p></div></div>
<p>This year, the community goal is to read 90% of our scanned scrolls. There are a number of prizes towards this goal:</p>
<ul>
<li>$200,000 - <strong><a href="#2024-grand-prize">2024 Grand Prize</a></strong> - Read 90% of 4 scrolls from Scrolls 1-5</li>
<li>$100,000 - <strong><a href="#first-automated-segmentation-prize">First Automated Segmentation Prize</a></strong> - Reproduce the 2023 Grand Prize result but faster</li>
<li>4 x $60,000 - <strong><a href="#3-first-letters-prizes--first-title-prize">First Letters and First Title Prizes</a></strong> - Find the title of Scroll 1, or first letters in Scrolls 2, 3, or 4</li>
<li>$350,000 - <strong><a href="#monthly-progress-prizes">Monthly Progress Prizes</a></strong> - Open ended prizes from $1,000-20,000</li>
</ul>
<p>Details below! This is a lot of information, and we want to help - bring your questions to our <a href="https://discord.com/invite/uTfNwwecCQ" target="_blank" rel="noopener noreferrer">Discord community</a> or to <a href="mailto:team@scrollprize.org" target="_blank" rel="noopener noreferrer">our team</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2024-grand-prize">2024 Grand Prize<a href="#2024-grand-prize" class="hash-link" aria-label="Direct link to 2024 Grand Prize" title="Direct link to 2024 Grand Prize">‚Äã</a></h2>
<p>The $200,000 2024 Grand Prize will go to the first person or team to read 90% of 4 scrolls from Scrolls 1-5.</p>
<p>In 2023, we got from 0% to 5% of one scroll. Reading 90% of four scrolls will lay the foundation to read all 300 scrolls. The 2024 criteria are designed to be as permissive, flexible, or favorable as possible within the high level objective of reading the scrolls. Favorable adjustments will be made if required (for example, if it is discovered conclusively that a scroll does not contain writing). Our mission is to read the scrolls. We want to award this prize!!</p>
<details class="submission-details" class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Submission criteria and requirements</summary><div><div class="collapsibleContent_i85q"><ol>
<li><strong>Segmentation</strong></li>
</ol><ul>
<li>Identify the four scrolls in your submission.</li>
<li>Compute the total surface area (in cm<sup>2</sup>) of papyrus sheets present in all four complete scrolls combined.<!-- -->
<ul>
<li>We may compute and specify this value ourselves as the year progresses, in which case you can skip this step.</li>
</ul>
</li>
<li>Compute the same measure for the papyrus sheets actually segmented in your submission. <strong>You must segment 90% or more of the total from all scrolls</strong> (not per-scroll).</li>
<li>Segments should be flattened and shown in 2D as if the scroll were unwrapped. Each scroll is ideally captured by a single segmentation (or each connected component of the scroll) rather than separate overlapping segmentations.</li>
<li>Segments should pass geometric sanity checks; for example, no self-intersections. We will evaluate <a href="https://arxiv.org/abs/2007.15551" target="_blank" rel="noopener noreferrer">stretch metrics</a> to measure distortion.</li>
</ul><ol start="2">
<li><strong>Ink detection</strong></li>
</ol><ul>
<li>Generate ink predictions on the segments.</li>
<li>The entire submission is too large to transcribe quickly, so the papyrological team will evaluate each line as:<!-- -->
<ul>
<li>‚úÖ <strong>readable</strong> (could read 85% of the characters),</li>
<li>‚ùå <strong>not readable</strong> (couldn&#x27;t),</li>
<li>üü° <strong>maybe</strong> (would have to stop and actually do the transcription to determine), or</li>
<li>üî∑ <strong>incomplete</strong> (line incomplete due to the physical boundaries of the scroll)</li>
</ul>
</li>
<li>90% of the total complete lines (incomplete lines will not be judged) must be either üü° <strong>maybe</strong> or ‚úÖ <strong>readable</strong>. Multiple papyrologists may review each line, in which case ties will be broken favorably towards the submission.</li>
</ul><p>As a baseline, here&#x27;s how the 2023 Grand Prize banner would have scored:</p><div class="mb-4"><img src="/img/2024-prizes/GP_scores_sample.webp" class="w-[80%]"><figcaption class="mt-[-6px]">Part of the 2023 Grand Prize banner scored using this rubric (<a href="/img/2024-prizes/2023_GP_banner_lines_score.webp">full banner</a>).</figcaption></div><p>Total lines: 240. Complete lines: 206. Passing lines: 137. Pass rate: 137 / 206 = <strong>67% (needs to be 90%)</strong>.</p><p>More and larger segmentations are needed, as well as improvements to ink recovery. Already both fronts are moving forward!</p><p>Multiple submissions are permitted, and we can provide feedback for early or partial submissions. If no team meets the above criteria by the deadline, Vesuvius Challenge may award the prize to the team(s) that were closest. These and other awards are at the sole discretion of Vesuvius Challenge.</p><p>The deadline is 11:59 pm Pacific December 31, 2024. When you are ready, see the <a href="/deepast/2024_gp_submissions">submission instructions</a>.</p></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="first-automated-segmentation-prize">First Automated Segmentation Prize<a href="#first-automated-segmentation-prize" class="hash-link" aria-label="Direct link to First Automated Segmentation Prize" title="Direct link to First Automated Segmentation Prize">‚Äã</a></h2>
<p>We&#x27;re awarding $100,000 to the first team to autosegment the 2023 Grand Prize region in Scroll 1, with quality matching or exceeding the manual segmentation from 2023 (comparable legibility and number of letters recovered). A second place $12,500 prize is also available for the second team to achieve this.</p>
<p>The <a href="/deepast/grandprize">2023 Grand Prize</a> showed that we can extract significant regions of text from inside a sealed Herculaneum scroll - but to scale up, these methods need to be significantly faster and cheaper. This prize asks you to reproduce a year of work in much less time. We believe this is possible using improved automation!</p>
<div class="mb-4"><img src="/img/2024-prizes/gp-segment-outlines.webp" class="w-[100%]"><figcaption class="mt-[-6px]">The 2023 Grand Prize banner segments that this prize aims to reproduce.</figcaption></div>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Submission criteria and requirements</summary><div><div class="collapsibleContent_i85q"><p>We will judge the segmentation by the following criteria:</p><ul>
<li><strong>Inputs.</strong> Maximum 4 hours of human input and 48 hours of compute (in any order). Existing 2023 Grand Prize segments (and overlapping ones) represent significant human input, so can only be used as inputs or training data if memorization is eliminated. Reach out to us for approval if you want to do this. Segments from elsewhere in the scroll can be used!</li>
<li><strong>Outputs.</strong> Our Segmentation, Technical, and Papyrological Teams will evaluate the segmentation:<!-- -->
<ul>
<li><strong>Geometric checks:</strong> Single continuous segmentation. Manifold. No self-intersections. Can exceed 2023 Grand Prize banner, but must cover 95% of the 2023 Grand Prize banner.</li>
<li><strong>Segmentation accuracy.</strong> Assessed by the Segmentation Team and the Technical Team.</li>
<li><strong>Flattening.</strong> You don‚Äôt necessarily have to implement flattening (it is provided in at least Volume Cartographer), but if you do, it should be comparable to 2023 Grand Prize results.</li>
<li><strong>Ink detection.</strong> Comparable or better ink detection as determined by our Papyrological Team. You can use the open source 2023 Grand Prize winning solution for this.</li>
</ul>
</li>
</ul><p>A submission requires:</p><ul>
<li><strong>Ink detection image.</strong> Submissions must include an image of the virtually unwrapped segment, showing visible and legible text.<!-- -->
<ul>
<li>Submit a single static image showing the text region. Images must be generated programmatically, as direct outputs of CT data inputs, and should not contain manual annotations of characters or text. Using annotations as training data is OK if they are not memorized by the model, for example if you use k-fold validation.</li>
<li>Include a scale bar on the submission image.</li>
<li>You may use open source methods (for example the 2023 Grand Prize winning submission) for ink detection.</li>
</ul>
</li>
<li><strong>Texture image.</strong> Include a texture image showing the papyrus fiber structure of the segmented region.<!-- -->
<ul>
<li>You may use Volume Cartographer‚Äôs <code>vc_render</code> or other existing tools to do this.</li>
<li>This image must be aligned with the above ink detection image and have the same dimensions.</li>
</ul>
</li>
<li><strong>Segmentation files.</strong> Provide the actual segmentation.<!-- -->
<ul>
<li>Expected format: A mesh file along with a UV map defining the flattening. If your approach uses another representation or parameterization, that is probably fine - feel free to ask us!</li>
</ul>
</li>
<li><strong>Proof of work.</strong> Your result should be reproducible using approximately 4 hours of human input and 48 hours of compute time.<!-- -->
<ul>
<li>Provide evidence of this. For example, a video recording the manual parts of your process.</li>
</ul>
</li>
<li><strong>Methodology.</strong> A detailed technical description of how your solution works. We need to be able to reproduce your work, so please make this as easy as possible:<!-- -->
<ul>
<li>For fully automated software, consider a Docker image that we can easily run to reproduce your work, and please include system requirements.</li>
<li>For software with a human in the loop, please provide written instructions and a video explaining how to use your tool. We‚Äôll work with you to learn how to use it, but we‚Äôd like to have a strong starting point.</li>
<li>Please include an easily accessible link from which we can download it.</li>
</ul>
</li>
</ul><p>Submissions must be made by 11:59pm Pacific, December 31, 2024. Make your submission using <a href="https://forms.gle/PyrriG8XFut7kqJeA" target="_blank" rel="noopener noreferrer">this form</a>.</p></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-first-letters-prizes--first-title-prize">3 First Letters Prizes + First Title Prize<a href="#3-first-letters-prizes--first-title-prize" class="hash-link" aria-label="Direct link to 3 First Letters Prizes + First Title Prize" title="Direct link to 3 First Letters Prizes + First Title Prize">‚Äã</a></h2>
<p>We‚Äôre issuing 3 more First Letters prizes, as well as a First Title Prize!</p>
<ul>
<li>First Title in Scroll 1: $60,000 1st place, $15,000 2nd place</li>
<li>First Letters in Scroll 2: $60,000 1st place, $15,000 2nd place</li>
<li>First Letters in Scroll 3: $60,000 1st place, $15,000 2nd place</li>
<li>First Letters in Scroll 4: $60,000 1st place, $15,000 2nd place</li>
</ul>
<div class="mb-4"><img src="/img/firstletters/purple_card-new.webp" class="w-[60%]"><figcaption class="mt-[-6px]">The first letters discovered in Scroll 1 in 2023.</figcaption></div>
<p>For Scrolls 2, 3, and 4, $60,000 will be awarded to the first team that uncovers 10 letters within a single area of 4 cm<sup>2</sup>, and open sources their methods and results (<em><strong>after</strong></em> winning the prize). $15,000 is also available for the second team to meet this bar.</p>
<p>We are also awarding $60,000 to the first team to unveil the title of Scroll 1, and $15,000 to the second team to find it before it is announced.
For more information about where to look for the title, see the <a href="https://scrollprize.substack.com/p/30k-first-title-prize" target="_blank" rel="noopener noreferrer">announcement of the First Title prize</a>.</p>
<p>The purpose of these prizes is to close the gap between our current state of the art and the seriously challenging 2024 Grand Prize. Last year we showed it is possible to recover text from a single rolled scroll. Generalizing these methods across multiple scrolls and scans will make them more robust, which will be needed to read the complete library.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Submission criteria and requirements</summary><div><div class="collapsibleContent_i85q"><ul>
<li><strong>Image.</strong> Submissions must be an image of the virtually unwrapped segment, showing visible and legible text.<!-- -->
<ul>
<li>Submit a single static image showing the text region. Images must be generated programmatically, as direct outputs of CT data inputs, and should not contain manual annotations of characters or text.</li>
<li>For the First Title Prize, please illustrate the ink predictions in spatial context of the title search, similar to what is <a href="https://scrollprize.substack.com/p/30k-first-title-prize" target="_blank" rel="noopener noreferrer">shown here</a>. You <strong>do not</strong> have to read the title yourself, but just have to produce an image of it that our team of papyrologists are able to read.</li>
<li>Specify which scroll the image comes from. For multiple scrolls, please make multiple submissions.</li>
<li>Include a scale bar showing the size of 1 cm on the submission image.</li>
<li>Specify the 3D position of the text within the scroll. The easiest way to do this is to provide the segmentation file (or the segmentation ID, if using a public segmentation).</li>
</ul>
</li>
<li><strong>Methodology.</strong> A detailed technical description of how your solution works. We need to be able to reproduce your work, so please make this as easy as possible:<!-- -->
<ul>
<li>For fully automated software, consider a Docker image that we can easily run to reproduce your work, and please include system requirements.</li>
<li>For software with a human in the loop, please provide written instructions and a video explaining how to use your tool. We‚Äôll work with you to learn how to use it, but we‚Äôd like to have a strong starting point.</li>
<li>Please include an easily accessible link from which we can download it.</li>
</ul>
</li>
<li><strong>Hallucination mitigation.</strong> If there is any risk of your model hallucinating results, please let us know how you mitigated that risk. Tell us why you are confident that the results you are getting are real.<!-- -->
<ul>
<li>We strongly discourage submissions that use window sizes larger than 0.5x0.5 mm to generate images from machine learning models. This corresponds to 64x64 pixels for 8 ¬µm scans. If your submission uses larger window sizes, we may reject it and ask you to modify and resubmit.</li>
</ul>
</li>
<li><strong>Other information.</strong> Feel free to include any other things we should know.</li>
</ul><p>Your submission will be reviewed by the review teams to verify technical validity and papyrological plausibility and legibility.
Just as with the Grand Prize, please <strong>do not</strong> make your discovery public until winning the prize. We will work with you to announce your findings.</p><p>Submissions must be made by 11:59pm Pacific, December 31, 2024. Make your submission using <a href="https://forms.gle/hQtXEtG95zpvfKDd6" target="_blank" rel="noopener noreferrer">this form</a>.</p></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monthly-progress-prizes">Monthly Progress Prizes<a href="#monthly-progress-prizes" class="hash-link" aria-label="Direct link to Monthly Progress Prizes" title="Direct link to Monthly Progress Prizes">‚Äã</a></h2>
<p>We‚Äôre awarding an estimated $350,000 this year in monthly progress prizes for submissions that get us closer to reading 90% of four scrolls. These prizes are more open-ended, and we have a wishlist to provide some ideas. If you are new to the project, this is a great place to start. Progress prizes will be awarded at a range of levels based on the contribution:</p>
<ul>
<li>Gold Aureus: $20,000 (estimated 4-8 per year) ‚Äì for major contributions</li>
<li>Denarius: $10,000 (estimated 10-15 per year)</li>
<li>Sestertius: $2,500 (estimated 25 per year)</li>
<li>Papyrus: $1,000 (estimated 50 per year)</li>
</ul>
<p>We favor submissions that:</p>
<ul>
<li>Are <strong>released or open-sourced early</strong>. Tools released earlier have a higher chance of being used for reading the scrolls than those released the last day of the month.</li>
<li>Actually <strong>get used</strong>. We‚Äôll look for signals from the community: questions, comments, bug reports, feature requests. Our Segmentation Team will publicly provide comments on tools they use.</li>
<li>Are <strong>well documented</strong>. It helps a lot if relevant documentation, walkthroughs, images, tutorials or similar are included with the work so that others can use it!</li>
</ul>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Our Wish List (progress prize ideas!)</summary><div><div class="collapsibleContent_i85q"><p>These encompass a wide range of award levels. Check back as we‚Äôll update this list!</p><ul>
<li><strong>Ink refinement:</strong> Other prizes target <em>new</em> ink findings, but we also want to see improvements to existing passages where ink recovery is poor</li>
<li><strong>3D/volumetric ink detection:</strong> and other creative approaches</li>
<li><strong>Improved documentation:</strong> updates to tutorials and introductions<!-- -->
<ul>
<li>Pull requests to <a href="https://github.com/ScrollPrize/scrollprize-website" target="_blank" rel="noopener noreferrer">our website</a> or standalone resources both accepted!</li>
</ul>
</li>
<li><strong>Volume Cartographer</strong> - <a href="https://github.com/educelab/volume-cartographer" target="_blank" rel="noopener noreferrer">EduceLab</a>, <a href="https://github.com/spacegaier/volume-cartographer" target="_blank" rel="noopener noreferrer">spacegaier fork</a>
<ul>
<li>Multi-axes viewports &amp; segmentation<!-- -->
<ul>
<li>Enable 3D (XYZ) inspection of scroll volume (similar to <a href="https://github.com/KhartesViewer/khartes" target="_blank" rel="noopener noreferrer">Khartes</a>)</li>
<li>Allow free-3D (not just XYZ) rotation with updating viewports</li>
<li>Ability to continue segmentation at any angle of rotation</li>
</ul>
</li>
<li>Flattened preview of editable/extendable segments<!-- -->
<ul>
<li>Live or near-live rendering from OME-Zarr volume</li>
<li>Enable live or near-live inspection of fiber continuity (similar to <a href="https://github.com/KhartesViewer/khartes" target="_blank" rel="noopener noreferrer">Khartes</a>)</li>
</ul>
</li>
<li>Ink detection preview of editable/extendable segments<!-- -->
<ul>
<li>Live or near-live rendering from OME-Zarr</li>
<li>Live or near-live updated ink detection viewport, similar to the flattened segment preview</li>
<li>Interchangeable ink detection models</li>
</ul>
</li>
<li>Import and Display pointclouds in VC<!-- -->
<ul>
<li>For inspection</li>
<li>For segmentation</li>
</ul>
</li>
<li>Novel segmentation algorithms</li>
<li>GUI improvements</li>
<li>Bug fixes<!-- -->
<ul>
<li>Eliminate dot residue: dots that were moved occasionally appear to remain in their original location until a view reset</li>
<li>Ability to save active changes to disk while Segmentation Tool is active</li>
<li>Disable re-saving for display-only segments; ask the user whether to save changed segments upon leaving &quot;compute&quot; mode</li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="https://github.com/schillij95/ThaumatoAnakalyptor" target="_blank" rel="noopener noreferrer">ThaumatoAnakalyptor</a>:</strong>
<ul>
<li>Better solution for sheet stitching</li>
<li>Improved point cloud extraction</li>
<li>Improved mesh reconstruction</li>
<li>More information in the <a href="https://github.com/schillij95/ThaumatoAnakalyptor/blob/main/documentation/ThaumatoAnakalyptor___Technical_Report_and_Roadmap.pdf" target="_blank" rel="noopener noreferrer">roadmap</a></li>
</ul>
</li>
<li><strong>Compressed areas:</strong> improved segmentation methods for regions of compressed papyrus</li>
<li><strong>Live previews:</strong> show live segment previews during segmentation and allow refinements to update the preview</li>
<li><strong>Scan analysis:</strong> analyze multi-energy/multi-resolution scans to identify optimal scan settings</li>
<li><strong>Volume registration:</strong> automated deformable techniques to align different scans (resolution, energy) of the same object</li>
<li><strong>Visualization tools</strong></li>
<li><strong>Segmentation inspection tools</strong></li>
<li><strong>Performance improvements:</strong> so these steps can handle larger segmentations:<!-- -->
<ul>
<li>Segmentation</li>
<li>Flattening</li>
<li>Rendering</li>
<li>Ink detection</li>
</ul>
</li>
</ul></div></div></details>
<p>Submissions are evaluated monthly, and multiple submissions/awards per month are permitted. The next deadline is 11:59pm Pacific, December 31, 2024! When ready, please submit using <a href="https://forms.gle/oCmaaAMrZuAEQ68W6" target="_blank" rel="noopener noreferrer">this form</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="terms-and-conditions">Terms and Conditions<a href="#terms-and-conditions" class="hash-link" aria-label="Direct link to Terms and Conditions" title="Direct link to Terms and Conditions">‚Äã</a></h2>
<p>Prizes are awarded at the sole discretion of Scroll Prize Inc. and are subject to review by our Technical Team, Segmentation Team, and Papyrological Team. We may issue more or fewer awards based on the spirit of the prize and the received submissions. You agree to make your method open source if you win a prize. It does not have to be open source at the time of submission, but you have to make it open source under a permissive license to accept the prize. Submissions for the First Automated Segmentation Prize, each First Letters Prize, the First Title Prize, and the 2024 Grand Prize will close once the winner is announced and their methods are open sourced. Scroll Prize Inc. reserves the right to modify prize terms at any time in order to more accurately reflect the spirit of the prize as designed. Prize winner must provide payment information to Scroll Prize Inc. within 30 days of prize announcement to receive prize.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/28_2024_prizes.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Overview</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/deepast/get_started">Getting Started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 Deep Past Challenge.</div></div></div></footer></div>
</body>
</html>