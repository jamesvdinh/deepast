"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3451],{3313:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"tutorial5","title":"Tutorial: Ink Detection","description":"<meta","source":"@site/docs/07_tutorial5.md","sourceDirName":".","slug":"/tutorial5","permalink":"/deepast/tutorial5","draft":false,"unlisted":false,"editUrl":"https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/07_tutorial5.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Tutorial: Ink Detection","sidebar_label":"Ink Detection","hide_table_of_contents":true}}');var n=s(4848),r=s(8453),a=s(9443);const l={title:"Tutorial: Ink Detection",sidebar_label:"Ink Detection",hide_table_of_contents:!0},o=void 0,c={},d=[];function m(e){const t={a:"a",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Head:s}=t;return s||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(s,{children:[(0,n.jsx)("html",{"data-theme":"dark"}),(0,n.jsx)("meta",{name:"description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,n.jsx)("meta",{property:"og:type",content:"website"}),(0,n.jsx)("meta",{property:"og:url",content:"https://scrollprize.org"}),(0,n.jsx)("meta",{property:"og:title",content:"Deep Past Challenge"}),(0,n.jsx)("meta",{property:"og:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,n.jsx)("meta",{property:"og:image",content:"https://scrollprize.org/img/social/opengraph.jpg"}),(0,n.jsx)("meta",{property:"twitter:card",content:"summary_large_image"}),(0,n.jsx)("meta",{property:"twitter:url",content:"https://scrollprize.org"}),(0,n.jsx)("meta",{property:"twitter:title",content:"Deep Past Challenge"}),(0,n.jsx)("meta",{property:"twitter:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,n.jsx)("meta",{property:"twitter:image",content:"https://scrollprize.org/img/social/opengraph.jpg"})]}),"\n","\n",(0,n.jsx)(a.L,{highlightId:5}),"\n",(0,n.jsx)(t.p,{children:"This tutorial gives a high-level overview on ink detection methods.\nSince this tutorial was written, ink detection has successfully recovered text from inside the rolled scrolls.\nThe technical principles remain the same as what is described here."}),"\n",(0,n.jsxs)(t.p,{children:["The tutorial can be followed by the more hands-on notebooks ",(0,n.jsx)(t.a,{href:"https://www.kaggle.com/code/jpposma/vesuvius-challenge-ink-detection-tutorial",children:"on Kaggle"})," and ",(0,n.jsx)(t.a,{href:"https://colab.research.google.com/github/ScrollPrize/vesuvius/blob/main/notebooks/example2_ink_detection.ipynb",children:"a more recent version on Colab"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"Ink detection is the task of taking data from a 3D X-ray scan around a papyrus surface, and identifying the locations of the inked parts of the papyrus."}),"\n",(0,n.jsx)(t.p,{children:"This is where one of the difficulties of the Herculaneum Papyri comes in: the ink and the papyrus have very similar densities, making it hard to detect ink in 3D X-ray scans."}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Campfire & En-Gedi scrolls:"})," Dense ink shows up as brighter voxels in 3D X-ray scans, so ink detection can be done by taking the brightest pixel in some voxel region."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Herculaneum scrolls & fragments:"})," Ink is less directly visible in 3D X-ray scans, but there does seem to be data there. Machine learning models can detect it, and humans can sometimes see subtle textural patterns in the data."]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"In the video below Dr. Seales talks about how ink detection got started for the Herculaneum scrolls:"}),"\n",(0,n.jsx)("iframe",{className:"w-[100%] aspect-video mb-4",src:"https://www.youtube.com/embed/g-7-Xg75CCI?start=5930",title:"YouTube video player",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0}),"\n",(0,n.jsxs)(t.p,{children:["Not only can machine learning models detect the ink, on occasion we can see the ink directly in the 3D X-ray volumes. Here are some examples, with slices from the 3D surface volumes on the left, and infrared photos showing ink on the right (from the ",(0,n.jsx)(t.a,{href:"https://arxiv.org/abs/2304.02084",children:"data paper"}),"):"]}),"\n",(0,n.jsx)("figure",{className:"",children:(0,n.jsx)("img",{src:"/img/tutorials/ink2-alpha.webp"})}),"\n",(0,n.jsxs)("figure",{className:"",children:[(0,n.jsx)("img",{src:"/img/tutorials/ink1-alpha.webp"}),(0,n.jsx)("figcaption",{className:"mt-0",children:"Ink visible in 3D surface volumes (left: 3D volume slice; right: infrared photo), found by Stephen Parsons"})]}),"\n",(0,n.jsx)(t.p,{children:"You have to look closely, but the shapes are visible!"}),"\n",(0,n.jsxs)(t.p,{children:["Discoveries from the community have found widespread examples of visible ink like this inside the intact scrolls. In particular the ",(0,n.jsx)(t.a,{href:"https://caseyhandmer.wordpress.com/2023/08/05/reading-ancient-scrolls/",children:"\u201ccrackle pattern\u201d"})," discovered by Casey Handmer has proven useful, and inspired a number of labeling approaches that successfully produce models capable of detecting ink in the scrolls."]}),"\n",(0,n.jsx)(t.p,{children:"For the purposes of the tutorial, we will use the fragment datasets, which contain ground truth ink labels made using infrared photography of the exposed writing on the surface."}),"\n",(0,n.jsx)(t.p,{children:"At a high level, training on a fragment works like this:"}),"\n",(0,n.jsx)("figure",{className:"",children:(0,n.jsx)("img",{src:"/img/tutorials/ml-overview-alpha.webp"})}),"\n",(0,n.jsx)(t.p,{children:"From a fragment (a) we obtain a 3D volume (b), from which we segment a mesh (c), around which we sample a surface volume (d). We also take an infrared photo (e) of the fragment, which we align (f) with the surface volume, and then manually turn into a binary label image (g)."}),"\n",(0,n.jsx)(t.p,{children:"We train this model by picking a pixel in the binary label image, and sampling a subvolume around the same coordinates from the surface volume. We then backpropagate the known label data to update the model weights:"}),"\n",(0,n.jsx)("figure",{children:(0,n.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] ",poster:"/img/tutorials/ink-training-anim3-dark.webp",children:(0,n.jsx)("source",{src:"/img/tutorials/ink-training-anim3-dark.webm",type:"video/webm"})})}),"\n",(0,n.jsx)(t.p,{children:"We can then use the model to predict what a label image would have looked like, from different input data than you have trained on."}),"\n",(0,n.jsx)("figure",{children:(0,n.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%]",poster:"/img/tutorials/ink-detection-anim3-dark.webp",children:(0,n.jsx)("source",{src:"/img/tutorials/ink-detection-anim3-dark.webm",type:"video/webm"})})}),"\n",(0,n.jsxs)(t.p,{children:["Of course, in reality the label image on the right doesn\u2019t come out perfectly. Stephen Parsons\u2019 ",(0,n.jsx)(t.a,{href:"https://github.com/educelab/ink-id/",children:"ink-id"})," program is one example of an ML-based approach. It produces outputs like this (showing different training epochs in k-fold training/prediction):"]}),"\n",(0,n.jsxs)("figure",{children:[(0,n.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] max-w-[238px]",poster:"/img/landing/fragment-training2.webp",children:(0,n.jsx)("source",{src:"/img/landing/fragment-training2.webm",type:"video/webm"})}),(0,n.jsx)("figcaption",{className:"mt-0",children:"A model learning to detect ink on a fragment, showing different training epochs"})]}),"\n",(0,n.jsx)(t.p,{children:"When running ink-id on all the public fragments, the results look like this (prediction left, infrared right):"}),"\n",(0,n.jsxs)("figure",{className:"",children:[(0,n.jsxs)("table",{className:"w-[100%] max-w-[400px]",children:[(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f1_composite.webp"})}),(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f1_ir.webp"})})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f2_composite.webp"})}),(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f2_ir.webp"})})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f3_composite.webp"})}),(0,n.jsx)("td",{className:"w-50%",children:(0,n.jsx)("img",{className:"w-[100%]",src:"/img/tutorials/f3_ir.webp"})})]})]}),(0,n.jsx)("figcaption",{className:"mt-0",children:"Predicted label images from ink-id (left); infrared photos (right)"})]}),"\n",(0,n.jsx)(t.p,{children:"As you can see, some letters can be clearly seen, others not at all, and a lot of letters are somewhere in between. All fragments also have \u201chidden layers\u201d: pieces of papyrus that are fused to the backs of the fragments. Running the machine model on those reveals some previously unseen letters:"}),"\n",(0,n.jsxs)("div",{className:"flex flex-wrap items-end max-w-[500px] mb-4",children:[(0,n.jsx)("figure",{className:"w-[33%]",children:(0,n.jsx)("img",{src:"/img/tutorials/f1_hidden_composite.webp"})}),(0,n.jsx)("figure",{className:"w-[33%]",children:(0,n.jsx)("img",{src:"/img/tutorials/f2_hidden_composite.webp"})}),(0,n.jsx)("figure",{className:"w-[33%]",children:(0,n.jsx)("img",{src:"/img/tutorials/f4_hidden_composite_partially_redacted.webp"})}),(0,n.jsx)("figcaption",{className:"mt-0",children:"\u201cHidden layers\u201d of papyrus, partially revealed by machine learning."})]}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.a,{href:"https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/",children:"Ink Detection Progress Prize on Kaggle"})," was all about creating the best possible machine learning model for detecting ink within the fragments. Since then newer models have successfully uncovered ink in full scrolls (the ",(0,n.jsx)(t.a,{href:"firstletters",children:"First Letters Prize"}),", and then the ",(0,n.jsx)(t.a,{href:"grandprize",children:"2023 Grand Prize"}),")."]}),"\n",(0,n.jsxs)(t.p,{children:["So how can a machine learning model detect ink? In the electron microscope images below (from the paper ",(0,n.jsx)(t.a,{href:"https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0215775&type=printable",children:"From invisibility to readability: Recovering the ink of Herculaneum"}),"), you can clearly see the difference between the inked and non-inked regions. We suspect that machine learning models are able to learn some of these features from the 3D X-ray scans."]}),"\n",(0,n.jsxs)("figure",{children:[(0,n.jsx)("a",{href:"/img/tutorials/sem.webp",target:"_blank",children:(0,n.jsx)("img",{src:"/img/tutorials/sem-alpha.webp",className:"w-[100%]"})}),(0,n.jsxs)("figcaption",{className:"mt-0",children:["Electron microscope pictures from the top (A and B) and the side (C) ",(0,n.jsx)("a",{href:"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0215775",children:"(source)"})]})]}),"\n",(0,n.jsx)("div",{children:"The main challenges for ink detection are:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Model performance, getting more letters to be legible."}),"\n",(0,n.jsx)(t.li,{children:"Applying these models to the full scrolls."}),"\n",(0,n.jsx)(t.li,{children:"Reverse engineering the models to better understand the kind of patterns they are using to detect ink."}),"\n",(0,n.jsx)(t.li,{children:"Creating more ground truth data (e.g. \u201ccampfire scrolls\u201d or synthetic data)."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["Now let\u2019s create a model! This part of the tutorial is over ",(0,n.jsx)(t.a,{href:"https://www.kaggle.com/code/jpposma/vesuvius-challenge-ink-detection-tutorial",children:"on Kaggle as a notebook"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["To run more advanced models on the ",(0,n.jsx)(t.a,{href:"data_segments",children:"scroll segments"}),", check out the winning code from the ",(0,n.jsx)(t.a,{href:"firstletters",children:"First Letters Prize"})," and the ",(0,n.jsx)(t.a,{href:"grandprize",children:"2023 Grand Prize"}),"."]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(m,{...e})}):m(e)}},8453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>l});var i=s(6540);const n={},r=i.createContext(n);function a(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),i.createElement(r.Provider,{value:t},e.children)}},9443:(e,t,s)=>{s.d(t,{L:()=>n});s(6540);var i=s(4848);function n({highlightId:e}={}){return(0,i.jsxs)("div",{className:"mx-[-16px] sm:mx-0 flex flex-wrap items-start mb-4 text-center justify-center sm:justify-start",children:[(0,i.jsxs)("a",{href:"/tutorial1",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(2==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,i.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-scanning-small.webp",children:(0,i.jsx)("source",{src:"/img/tutorial-thumbs/top-scanning-small.webm",type:"video/webm"})}),(0,i.jsx)("div",{className:"text-sm",children:"Scanning"})]}),(0,i.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,i.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,i.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,i.jsxs)("a",{href:"/tutorial3",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(3==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,i.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-segmentation-small.webp",children:(0,i.jsx)("source",{src:"/img/tutorial-thumbs/top-segmentation-small.webm",type:"video/webm"})}),(0,i.jsx)("div",{className:"text-sm",children:"Segmentation and Flattening"})]}),(0,i.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,i.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,i.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,i.jsxs)("a",{href:"/tutorial4",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(4==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,i.jsx)("img",{className:"max-h-[300px] m-2",src:"/img/tutorials/fisherman.webp"}),(0,i.jsx)("div",{className:"text-sm",children:"Segmentation - a different approach"})]}),(0,i.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,i.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,i.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,i.jsxs)("a",{href:"/tutorial5",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(5==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,i.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-prediction-small3.webp",children:(0,i.jsx)("source",{src:"/img/tutorial-thumbs/top-prediction-small.webm",type:"video/webm"})}),(0,i.jsx)("div",{className:"text-sm",children:"Ink Detection"})]})]})}}}]);