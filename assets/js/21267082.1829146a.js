"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5283],{5537:(e,n,t)=>{t.d(n,{A:()=>y});var s=t(6540),a=t(4164),i=t(5627),r=t(6347),o=t(372),l=t(604),c=t(1861),d=t(8749);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return h(e).map((({props:{value:e,label:n,attributes:t,default:s}})=>({value:e,label:n,attributes:t,default:s})))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p({value:e,tabValues:n}){return n.some((n=>n.value===e))}function m({queryString:e=!1,groupId:n}){const t=(0,r.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(a),(0,s.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})}),[a,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=u(e),[r,l]=(0,s.useState)((()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i}))),[c,h]=m({queryString:t,groupId:a}),[f,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,d.Dv)(n);return[t,(0,s.useCallback)((e=>{n&&a.set(e)}),[n,a])]}({groupId:a}),g=(()=>{const e=c??f;return p({value:e,tabValues:i})?e:null})();(0,o.A)((()=>{g&&l(g)}),[g]);return{selectedValue:r,selectValue:(0,s.useCallback)((e=>{if(!p({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),x(e)}),[h,x,i]),tabValues:i}}var x=t(9136);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(4848);function v({className:e,block:n,selectedValue:t,selectValue:s,tabValues:r}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,i.a_)(),c=e=>{const n=e.currentTarget,a=o.indexOf(n),i=r[a].value;i!==t&&(l(n),s(i))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:r.map((({value:e,label:n,attributes:s})=>(0,b.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...s,className:(0,a.A)("tabs__item",g.tabItem,s?.className,{"tabs__item--active":t===e}),children:n??e},e)))})}function j({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find((e=>e.props.value===t));return e?(0,s.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function w(e){const n=f(e);return(0,b.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,b.jsx)(v,{...n,...e}),(0,b.jsx)(j,{...n,...e})]})}function y(e){const n=(0,x.A)();return(0,b.jsx)(w,{...e,children:h(e.children)},String(n))}},5724:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>d,default:()=>m,frontMatter:()=>c,metadata:()=>s,toc:()=>u});const s=JSON.parse('{"id":"tutorial2","title":"Tutorial: Representation","description":"<meta","source":"@site/docs/05_tutorial2.md","sourceDirName":".","slug":"/tutorial2","permalink":"/deepast/tutorial2","draft":false,"unlisted":false,"editUrl":"https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/05_tutorial2.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Tutorial: Representation","sidebar_label":"Representation","hide_table_of_contents":true}}');var a=t(4848),i=t(8453),r=t(9443),o=t(5537),l=t(9329);const c={title:"Tutorial: Representation",sidebar_label:"Representation",hide_table_of_contents:!0},d=void 0,h={},u=[{value:"Semantic Segmentation of Scroll Structures",id:"semantic-segmentation-of-scroll-structures",level:2},{value:"Installing nnUNet and its Dependencies",id:"installing-nnunet-and-its-dependencies",level:3},{value:"Data Preparation and Preprocessing",id:"data-preparation-and-preprocessing",level:3},{value:"Training",id:"training",level:3},{value:"Inference",id:"inference",level:3}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components},{Head:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(t,{children:[(0,a.jsx)("html",{"data-theme":"dark"}),(0,a.jsx)("meta",{name:"description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"og:type",content:"website"}),(0,a.jsx)("meta",{property:"og:url",content:"https://scrollprize.org"}),(0,a.jsx)("meta",{property:"og:title",content:"Deep Past Challenge"}),(0,a.jsx)("meta",{property:"og:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"og:image",content:"https://scrollprize.org/img/social/opengraph.jpg"}),(0,a.jsx)("meta",{property:"twitter:card",content:"summary_large_image"}),(0,a.jsx)("meta",{property:"twitter:url",content:"https://scrollprize.org"}),(0,a.jsx)("meta",{property:"twitter:title",content:"Deep Past Challenge"}),(0,a.jsx)("meta",{property:"twitter:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"twitter:image",content:"https://scrollprize.org/img/social/opengraph.jpg"})]}),"\n","\n",(0,a.jsx)(r.L,{highlightId:2}),"\n",(0,a.jsx)(n.p,{children:"The micro-CT scan converts the scroll into a volumetric grid of density measurements, providing a mathematical description with integer coordinates and average material densities in small adjacent cubes known as voxels."}),"\n",(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[75%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/rep_ash2vox.webp",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Physical scroll to voxel representation"})]})}),"\n",(0,a.jsx)(n.p,{children:"While the raw scan volume represents the maximum amount of information available, this dataset is huge and the papyrus surface is difficult to locate throughout the scroll. Mathematical calculations and machine learning can be used to manipulate the digital dataset into different representations that are easier to interpret both visually and computationally."}),"\n",(0,a.jsx)(n.p,{children:"Besides the raw scan volume, three types of representations have been explored:"}),"\n",(0,a.jsxs)(o.A,{children:[(0,a.jsxs)(l.A,{value:"Machine Learning Segmentation",label:"Machine Learning Segmentation",default:!0,children:[(0,a.jsx)("div",{className:"bg-gray-800 p-4 rounded mb-4",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("strong",{children:"Input:"})," 3D scan volume (voxels in .tif \u201cimage stack\u201d)",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("strong",{children:"Output:"})," binary 3D volume (voxels in .tif \u201cimage stack\u201d)"]})}),(0,a.jsx)(n.p,{children:"Machine learning segmentation involves separating the part of the papyrus we wish to work with from the rest of the scan. We have trained models on a number of different papyrus features, but our current methods focus on two structures that exist within the scroll: The recto (inside) or verso (outside) surfaces, and the fibers which make up the papyrus sheet itself."}),(0,a.jsx)(n.p,{children:"In the image below, the purple lines mark the recto surface, giving us a much more tractable representation to work with on downstream tasks than the scan itself."}),(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[75%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/s1_surface_pred.png",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Semantic segmentation of the recto surface"})]})})]}),(0,a.jsxs)(l.A,{value:"Traditional voxel processing",label:"Traditional voxel processing",default:!0,children:[(0,a.jsx)(n.h3,{id:""}),(0,a.jsx)("div",{className:"bg-gray-800 p-4 rounded mb-4",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("strong",{children:"Input:"})," 3D scan volume (voxels in .tif \u201cimage stack\u201d)",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("strong",{children:"Output:"})," new 3D volume (voxels in .tif \u201cimage stack\u201d)"]})}),(0,a.jsx)(n.p,{children:"Traditional image processing techniques have mostly been intended to aid manual segmentation, either to make the papyrus easier to interpret or to help segmentation algorithms follow the surface better. Popular techniques include thresholding, noise reduction, various colourization schemes, edge detection, and skeletonization."}),(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[75%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/rep_trad_voxel.webp",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Voxel to voxel representations using traditional techniques."})]})}),(0,a.jsx)(n.p,{children:"Most of these techniques have not been successful at improving segmentation."})]}),(0,a.jsxs)(l.A,{value:"Point Clouds",label:"Point Clouds",default:!0,children:[(0,a.jsx)("div",{className:"bg-gray-800 p-4 rounded mb-4",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("strong",{children:"Input:"})," 3D scan volume (voxels in .tif \u201cimage stack\u201d)",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("strong",{children:"Output:"})," point cloud (.ply or .obj)"]})}),(0,a.jsx)(n.p,{children:"The goal of point clouds is to extract a limited but useful subset of information from the raw scan volume. In the point cloud representation, calculations select voxels that describe the surface of a sheet."}),(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[75%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/rep_vox2pointcloud.webp",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Transforming voxels into a point cloud representation."})]})}),(0,a.jsx)(n.p,{children:"The fixed grid and density values are discarded, using traditional edge/surface detection to generate infinitesimal points characterized by 3D real coordinates. If needed, the discarded information can be reattached to the points to revert to the voxel representation."})]})]}),"\n",(0,a.jsx)(n.h2,{id:"semantic-segmentation-of-scroll-structures",children:"Semantic Segmentation of Scroll Structures"}),"\n",(0,a.jsx)(n.p,{children:"This guide will focus on utilizing machine learning to create semantic segmentations of recto surfaces, but the same methods apply to any of our other datasets, such as fibers -- simply swap out the training images and their associated labels. It is written using Ubuntu 22.04, but the guide should translate to most any version of Linux. Windows users are advised to use WSL2 and place their data within the Linux filesystem."}),"\n",(0,a.jsxs)(n.p,{children:["For this tutorial, we'll be using a library/framework known as ",(0,a.jsx)(n.a,{href:"https://github.com/MIC-DKFZ/nnUNet",children:"nnUNetv2"})]}),"\n",(0,a.jsx)(n.p,{children:"nnUNet is a well regarded and highly performant machine learning library with a focus on volumetric data within the medical domain, which has many parallels to our imaging modalities (namely xray tomography), and provides us with a fantastic baseline and starting point for experimentation."}),"\n",(0,a.jsxs)(n.p,{children:["This is a simplified guide, it is highly recommended to visit ",(0,a.jsx)(n.a,{href:"https://github.com/MIC-DKFZ/nnUNet",children:"the repository"}),"and read further after following this!"]}),"\n",(0,a.jsx)(n.h3,{id:"installing-nnunet-and-its-dependencies",children:"Installing nnUNet and its Dependencies"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Requirements"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Working CUDA installation -- ",(0,a.jsx)(n.a,{href:"https://developer.nvidia.com/cuda-downloads",children:"Installation instructions"})]}),"\n",(0,a.jsx)(n.li,{children:"Nvidia GPU with 10GB VRAM"}),"\n",(0,a.jsx)(n.li,{children:"32GB CPU RAM (although you may be able to get by with a bit less)"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"If you do not already have a virtual environment manager, install miniconda:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \n\nbash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 \n\nsource $HOME/miniconda3/bin/activate\n"})}),"\n",(0,a.jsx)(n.p,{children:"Create a virtual environment for nnUNet:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"conda create -n nnunet python=3.10\nconda activate nnunet\n"})}),"\n",(0,a.jsx)(n.p,{children:"Install Pip within the Conda Environment (this is done because PyTorch no longer creates Conda packages)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"conda install pip \n"})}),"\n",(0,a.jsx)(n.p,{children:"Install PyTorch"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Install nnUNetv2. if you wish to extend nnUNet, you can install in editable mode by following ",(0,a.jsx)(n.a,{href:"https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/installation_instructions.md#installation-instructions",children:"these instructions"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install nnunetv2\n"})}),"\n",(0,a.jsx)(n.h3,{id:"data-preparation-and-preprocessing",children:"Data Preparation and Preprocessing"}),"\n",(0,a.jsxs)(n.p,{children:["Download and extract the ",(0,a.jsx)(n.a,{href:"https://dl.ash2txt.org/community-uploads/bruniss/nnunet_models/nnUNet_raw/Dataset060_s1_s4_s5_patches_frangiedt_test.zip",children:"test dataset"}),". Choose a folder you'd like to store your nnUNet data in, and structure the folders like so, placing the dataset where indicated in the tree:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:".\n\u2514\u2500\u2500 nnUNet_data/\n    \u251c\u2500\u2500 nnUNet_raw/\n    \u2502   \u2514\u2500\u2500 Dataset060_s1_s4_s5_patches_frangiedt_test/\n    \u2502       \u251c\u2500\u2500 imagesTr\n    \u2502       \u251c\u2500\u2500 labelsTr\n    \u2502       \u2514\u2500\u2500 dataset.json\n    \u251c\u2500\u2500 nnUNet_preprocessed\n    \u2514\u2500\u2500 nnUNet_results\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You may notice by now that the images contain a suffix of _0000 while the labels do not. This suffix maps the images to a normalization scheme defined in ",(0,a.jsx)(n.code,{children:"dataset.json"}),", for more information if desired, visit ",(0,a.jsx)(n.a,{href:"https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/explanation_normalization.md",children:"this page"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["nnUNet requires some information about where you have stored this dataset, which it accesses from environment variables. Set them for the current session with the following commands. You can make them permanent if you'd like by adding the same lines to your ",(0,a.jsx)(n.code,{children:"~/.bashrc"})," file."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'export nnUNet_raw="/path/to/nnUNet_data/nnUNet_raw/"\nexport nnUNet_preprocessed="/path/to/nnUNet_data/nnUNet_preprocessed/"\nexport nnUNet_results="/path/to/nnUNet_data/nnUNet_results/"\n'})}),"\n",(0,a.jsx)(n.p,{children:"Now we can begin the preprocessing stage. nnUNet will gather some information about your dataset, verify there is nothing wrong with it, and extract it into uncompressed numpy arrays."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nnUNetv2_plan_and_preprocess -d 060 -pl nnUNetPlannerResEncM \n"})}),"\n",(0,a.jsx)(n.p,{children:"This can take a few minutes, and will print some information as it goes. It should finish without errors."}),"\n",(0,a.jsx)(n.h3,{id:"training",children:"Training"}),"\n",(0,a.jsxs)(n.p,{children:["With our data preprocessed we can finally begin training. The command here will begin a training run using dataset ",(0,a.jsx)(n.code,{children:"060"})," (the identifier for our test dataset), using the ",(0,a.jsx)(n.code,{children:"3d_fullres"})," configuration, and fold ",(0,a.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["nnUNet automatically creates 5 training folds for cross-validation on your dataset, and these splits can be modified in the ",(0,a.jsx)(n.code,{children:"splits_final.json"})," which is generated after initiating training. You can run without any cross-validation by specifying ",(0,a.jsx)(n.code,{children:"all"})," instead of a fold number."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nnUNetv2_train 060 3d_fullres 0 -p nnUNetResEncUNetMPlans\n"})}),"\n",(0,a.jsxs)(n.p,{children:["nnUNet will extract the dataset, and begin training at epoch 0. While training runs it will print the progress in the terminal. The EMA pseudo dice is computed on a random validation batch, and shouldn't be taken as gospel, but can provide a relative idea of how well the training is going, provided you have not used the ",(0,a.jsx)(n.code,{children:"all"})," fold."]}),"\n",(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[100%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/nnunet_training.png",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Training progress"})]})}),"\n",(0,a.jsxs)(n.p,{children:["nnUNet will also store a graph that can be used to assess the current training run located in ",(0,a.jsx)(n.code,{children:"/path/to/nnUNet_data/nnUNet_results/Dataset060_s1_s4_s5_patches_frangiedt_test/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres/fold_0/progress.png"})]}),"\n",(0,a.jsx)(n.p,{children:"This contains information about current learning rate and epoch time, but of particular interest is the top graph containing information about current psuedo dice and training and validation loss. Ideally, your bar should look like this -- where both training and validation loss are continuing to drop along similar paths."}),"\n",(0,a.jsxs)(n.p,{children:["Towards the end, you can see my training loss continues to drop while my validation loss levels out, this tells us that the model is no longer improving on the validation data, and is likely beginning to overfit to the training data. This is not necessarily bad, but you would not want these lines to diverge too greatly. You can end training at any time by pressing ",(0,a.jsx)(n.code,{children:"ctrl+c"})," in the terminal window, or allow training to continue for the default ",(0,a.jsx)(n.code,{children:"max_epochs"}),", which for nnUNet is 1,000."]}),"\n",(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[100%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"/img/tutorials/nnunet_progress.png",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"Training progress"})]})}),"\n",(0,a.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,a.jsxs)(n.p,{children:["To run inference with your newly trained model, you first need to ensure that the file names contain the same suffix as the ones you trained with. In our case, this is ",(0,a.jsx)(n.code,{children:"_0000"}),". ",(0,a.jsx)(n.strong,{children:"This example will run inference on all of P.Herc. 1667 to show the process of reconstructing the inference back to the original volume, you can simply download a single grid and run inference on only one if you only wish to see what the inference output looks like"}),"!"]}),"\n",(0,a.jsxs)(n.p,{children:["Download the ",(0,a.jsx)(n.a,{href:"https://dl.ash2txt.org/community-uploads/bruniss/scrolls/s4/grids/",children:"Scroll 4 grids"}),". These are 600x600x600 volumes, with the same naming convention as the original scroll grids, but with 50 voxels of overlap in all directions so that when we reconstruct it we can do so in a way that mitigates edge artifacts. Inference on this entire volume takes around 13 hours on two RTX 3090s."]}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["If you stopped your training early, you'll need to go into the results folder in your nnUNet_results path, and change one of the checkpoints to ",(0,a.jsx)(n.code,{children:"checkpoint_final.pth"})]})}),"\n",(0,a.jsx)(n.p,{children:"Place the data in any folder, and run the command"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nnUNetv2_predict -d 060 -f 0 -c 3d_fullres -i /path/to/input/grids -o /path/to/output/grids -p nnUNetResEncUNetMPlans\n"})}),"\n",(0,a.jsx)(n.p,{children:"for multi-gpu setups, you can run inference like so, repeating for as many gpus as you have available"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"CUDA_VISIBLE_DEVICES=0 nnUNetv2_predict d 060 -f 0 -c 3d_fullres -i /path/to/input/folder -o /path/to/output/folder -p nnUNetResEncUNetMPlans -num_parts 2 -part_id 0  &\n\nCUDA_VISIBLE_DEVICES=1 nnUNetv2_predict d 060 -f 0 -c 3d_fullres -i /path/to/input/folder -o /path/to/output/folder -p nnUNetResEncUNetMPlans -num_parts 2 -part_id 1 \n"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["To export the probabilities, append --save_probabilities to the end of the inference commands. This will save an npz and a pkl file, which you can turn into uint8 tifs with ",(0,a.jsx)(n.a,{href:"https://dl.ash2txt.org/community-uploads/bruniss/scrolls/helper-scripts/mp-softmax.py",children:"this script"})]})}),"\n",(0,a.jsxs)(n.p,{children:["Once inference has completed, download ",(0,a.jsx)(n.a,{href:"https://dl.ash2txt.org/community-uploads/bruniss/scrolls/helper-scripts/grids_to_zarr.py",children:"this script"}),", and in the main block, change ",(0,a.jsx)(n.code,{children:"map=False"})," to ",(0,a.jsx)(n.code,{children:"map=True"}),", and change ",(0,a.jsx)(n.code,{children:"Scroll='1'"})," to ",(0,a.jsx)(n.code,{children:"Scroll='4'"}),". This will modify the pixel values of your inference (which are simply 0 for background and 1 for foreground) to 0 and 255, and map your grids to the proper volume shape. You can modify ",(0,a.jsx)(n.code,{children:"max_workers"})," and ",(0,a.jsx)(n.code,{children:"chunk_size"})," if desired."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python grids_to_zarr.py\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This script will add your blocks to a zarr array of the proper shape, and trim the 50 voxel overlaps we added previously. The result of this is a single resolution zarr. It is recommended to create a multi-resolution ome-zarr , by using ",(0,a.jsx)(n.a,{href:"https://github.com/KhartesViewer/scroll2zarr/blob/main/zarr_to_ome.py",children:"this script"}),", written by Discord member @khartes_chuck."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python zarr_to_ome.py /path/to/single_resolution.zarr /path/to/multi_resolution_ome.zarr --algorithm max\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The final result is the predicted recto surface for the entirety of Scroll 4 that should be much easier to work with than the original scan data in downstream tasks. Volumes like this one are the inputs for methods used in our ",(0,a.jsx)(n.a,{href:"/segmentation",children:"next guide"})," on segmentation."]}),"\n",(0,a.jsx)("div",{className:"flex max-w-[400px]",children:(0,a.jsxs)("div",{className:"w-[100%] mb-2 mr-2",children:[(0,a.jsx)("img",{src:"img/virtual-unwrapping/surface-predictions.jpg",className:"w-[100%]"}),(0,a.jsx)("figcaption",{className:"mt-0",children:"A slice of a predicted surface volume"})]})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},9329:(e,n,t)=>{t.d(n,{A:()=>r});t(6540);var s=t(4164);const a={tabItem:"tabItem_Ymn6"};var i=t(4848);function r({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(a.tabItem,t),hidden:n,children:e})}},9443:(e,n,t)=>{t.d(n,{L:()=>a});t(6540);var s=t(4848);function a({highlightId:e}={}){return(0,s.jsxs)("div",{className:"mx-[-16px] sm:mx-0 flex flex-wrap items-start mb-4 text-center justify-center sm:justify-start",children:[(0,s.jsxs)("a",{href:"/tutorial1",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(2==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,s.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-scanning-small.webp",children:(0,s.jsx)("source",{src:"/img/tutorial-thumbs/top-scanning-small.webm",type:"video/webm"})}),(0,s.jsx)("div",{className:"text-sm",children:"Scanning"})]}),(0,s.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,s.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,s.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,s.jsxs)("a",{href:"/tutorial3",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(3==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,s.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-segmentation-small.webp",children:(0,s.jsx)("source",{src:"/img/tutorial-thumbs/top-segmentation-small.webm",type:"video/webm"})}),(0,s.jsx)("div",{className:"text-sm",children:"Segmentation and Flattening"})]}),(0,s.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,s.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,s.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,s.jsxs)("a",{href:"/tutorial4",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(4==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,s.jsx)("img",{className:"max-h-[300px] m-2",src:"/img/tutorials/fisherman.webp"}),(0,s.jsx)("div",{className:"text-sm",children:"Segmentation - a different approach"})]}),(0,s.jsxs)("div",{className:"hidden sm:flex mx-2 mb-2 flex-col items-center",children:[(0,s.jsx)("div",{className:"relative leading-[150px] py-4 w-[16px] text-center",children:"\u2192"}),(0,s.jsx)("div",{className:"text-sm",children:"\xa0"})]}),(0,s.jsxs)("a",{href:"/tutorial5",className:"mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl "+(5==e?"bg-[#fffefc30] hover:bg-[#fefefe45]":""),children:[(0,s.jsx)("video",{autoPlay:!0,playsInline:!0,loop:!0,muted:!0,className:"w-[100%] rounded-xl mb-2",poster:"/img/tutorial-thumbs/top-prediction-small3.webp",children:(0,s.jsx)("source",{src:"/img/tutorial-thumbs/top-prediction-small.webm",type:"video/webm"})}),(0,s.jsx)("div",{className:"text-sm",children:"Ink Detection"})]})]})}}}]);