"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4229],{2730:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"challenges","title":"Challenges","description":"<meta","source":"@site/docs/15_challenges.md","sourceDirName":".","slug":"/challenges","permalink":"/challenges","draft":false,"unlisted":false,"editUrl":"https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/15_challenges.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"title":"Challenges","hide_table_of_contents":true},"sidebar":"overviewSidebar","previous":{"title":"Get Started","permalink":"/get_started"},"next":{"title":"Challenges","permalink":"/challenges"}}');var i=t(4848),a=t(8453);t(6540);const l={title:"Challenges",hide_table_of_contents:!0},r=void 0,o={},d=[{value:"1. Task: Image segmentation + annotation",id:"1-task-image-segmentation--annotation",level:2},{value:"The Challenge",id:"the-challenge",level:3},{value:"Refining segmentation strategy",id:"refining-segmentation-strategy",level:4},{value:"Finalizing segmentation plan",id:"finalizing-segmentation-plan",level:4},{value:"Addressing dataset structure",id:"addressing-dataset-structure",level:4},{value:"Finalizing dataset size recommendations",id:"finalizing-dataset-size-recommendations",level:4},{value:"(1) How many sign instances should you annotate?",id:"1-how-many-sign-instances-should-you-annotate",level:3},{value:"(2) Handling overlapped or &quot;ligatured&quot; signs",id:"2-handling-overlapped-or-ligatured-signs",level:3},{value:"(3) Proposed dataset split",id:"3-proposed-dataset-split",level:3},{value:"(4) Annotation conventions (JSON, COCO-style)",id:"4-annotation-conventions-json-coco-style",level:3},{value:"(5) Quality-of-life tips",id:"5-quality-of-life-tips",level:3},{value:"Botton line",id:"botton-line",level:3}];function c(e){const n={a:"a",annotation:"annotation",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components},{Head:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t,{children:[(0,i.jsx)("html",{"data-theme":"dark"}),(0,i.jsx)("meta",{name:"description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,i.jsx)("meta",{property:"og:type",content:"website"}),(0,i.jsx)("meta",{property:"og:url",content:"https://scrollprize.org"}),(0,i.jsx)("meta",{property:"og:title",content:"Vesuvius Challenge"}),(0,i.jsx)("meta",{property:"og:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,i.jsx)("meta",{property:"og:image",content:"https://scrollprize.org/img/social/opengraph.jpg"}),(0,i.jsx)("meta",{property:"twitter:card",content:"summary_large_image"}),(0,i.jsx)("meta",{property:"twitter:url",content:"https://scrollprize.org"}),(0,i.jsx)("meta",{property:"twitter:title",content:"Vesuvius Challenge"}),(0,i.jsx)("meta",{property:"twitter:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,i.jsx)("meta",{property:"twitter:image",content:"https://scrollprize.org/img/social/opengraph.jpg"})]}),"\n",(0,i.jsx)(n.p,{children:"This page lists all the competition challenges:"}),"\n",(0,i.jsx)(n.h2,{id:"1-task-image-segmentation--annotation",children:"1. Task: Image segmentation + annotation"}),"\n",(0,i.jsx)(n.h3,{id:"the-challenge",children:"The Challenge"}),"\n",(0,i.jsx)(n.h4,{id:"refining-segmentation-strategy",children:"Refining segmentation strategy"}),"\n",(0,i.jsx)(n.p,{children:"To develop this answer, I'll focus on the dataset size needed for segmentation. With 200 classes of signs and variation in handwriting styles, each photo of tablets showing multiple signs means we need a solid number of annotated segments. A starting point would be around 1,000 instances per class for deep learning. I'll aim for a minimum of 60,000 segments, but top classes might require more. I'll also need to define proper development sets: train, validation, and test, and consider using active learning to improve iteratively. Should multi-sign segments be labeled as a single 'multi' class? I'll clarify this next."}),"\n",(0,i.jsx)(n.h4,{id:"finalizing-segmentation-plan",children:"Finalizing segmentation plan"}),"\n",(0,i.jsx)(n.p,{children:"For ground truth clarity, I'll recommend annotating multi-character clusters separately, with a \"cluster: true\" label and option to assign individual signs within, using instance segmentation with polygons. I'll propose a two-level dataset: individual sign boxes and cluster polygons. For dataset size, aiming for 50k-100k segments seems ideal to capture variance, with stratified sampling \u2014 70-80% for training. Targeting at least 200k instances for broad coverage. If the budget's tight, 80k-100k segments should suffice. I\u2019ll also look up literature to finalize segmentation sizes."}),"\n",(0,i.jsx)(n.h4,{id:"addressing-dataset-structure",children:"Addressing dataset structure"}),"\n",(0,i.jsx)(n.p,{children:"For clear evaluation, it\u2019s best to annotate multi-character clusters separately, adding a \u201ccluster: true\u201d attribute. Individual sign labels within clusters can also use instance segmentation with polygons. I\u2019ll recommend a two-level dataset: 1) individual sign bounding boxes; 2) cluster region polygons. To ensure proper sampling, a goal of 50,000\u2013100,000 signs, well distributed, with at least 100 instances for rare signs, seems ideal. For validation, I\u2019ll suggest stratified sampling \u2014 70-80% for training, 10-15% for validation and testing, aiming for around 200,000 instance masks."}),"\n",(0,i.jsx)(n.h4,{id:"finalizing-dataset-size-recommendations",children:"Finalizing dataset size recommendations"}),"\n",(0,i.jsx)(n.p,{children:"For a baseline challenge, I\u2019d suggest aiming for at least 200k sign masks, though if the annotation budget is tight, 80-100k segments should be sufficient. With 10,000 images and an average of 20 signs per photo, that\u2019ll give us 200k segments. A smaller dataset of 50k could also work. I\u2019ll recommend optional multi-sign segments, but they should include an attribute. The dataset spec should reference bounding polygons for signs. I'll prepare to cite references such as COCO and PASCAL VOC datasets for justification."}),"\n",(0,i.jsx)(n.h3,{id:"1-how-many-sign-instances-should-you-annotate",children:"(1) How many sign instances should you annotate?"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Reference set (for comparison)"}),(0,i.jsx)(n.th,{children:"# classes"}),(0,i.jsx)(n.th,{children:"# images"}),(0,i.jsx)(n.th,{children:"# instance masks"}),(0,i.jsx)(n.th,{children:"~instances / class"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"PASCAL-VOC 2012 (seg.)"}),(0,i.jsx)(n.td,{children:"20"}),(0,i.jsx)(n.td,{children:"\u22487 k"}),(0,i.jsx)(n.td,{children:"\u224820 k"}),(0,i.jsxs)(n.td,{children:["\u22481 k (",(0,i.jsx)(n.a,{href:"https://datasetninja.com/pascal-voc-2012?utm_source=chatgpt.com",children:"Dataset Ninja"}),")"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"MS-COCO 2017 (seg.)"}),(0,i.jsx)(n.td,{children:"80"}),(0,i.jsx)(n.td,{children:"\u2248164 k"}),(0,i.jsx)(n.td,{children:"\u22481.8 M"}),(0,i.jsxs)(n.td,{children:["\u224822 k (",(0,i.jsx)(n.a,{href:"https://datasetninja.com/coco-2017?utm_source=chatgpt.com",children:"Dataset Ninja"}),")"]})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Deep\u2011learning segmentation models begin to generalise once they see hundreds of masks per class and keep improving up to a few thousand. With 200 cuneiform signs, a realistic, competition\u2011ready target is:"}),"\n",(0,i.jsx)(n.p,{children:"\u2248 300\u20131\u202f000 masks\u202f/\u202fsign  \u2192  60\u202f000\u202f\u2013\u202f200\u202f000 total masks"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"If you can afford a single full pass over all 10\u202f000 photos (average 20\u201325 signs per face) you will land naturally in the upper half of that range, giving participants a dataset on par with modern benchmarks."})}),"\n",(0,i.jsx)(n.h3,{id:"2-handling-overlapped-or-ligatured-signs",children:'(2) Handling overlapped or "ligatured" signs'}),"\n",(0,i.jsx)(n.p,{children:"Because ancient scribes often wrote signs against or on top of each other, copy modern practice from COCO:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Annotate every sign you can separate"})," with its own polygon & category_id."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["When two or more signs are truly inseparable, draw ",(0,i.jsx)(n.strong,{children:"one tight polygon around the whole cluste"}),"r** and mark"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:'"iscrowd": 1        /* or  "cluster": true  */'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.em,{children:"Evaluation scripts then ignore that region for per\u2011instance scoring while still letting models learn its shape."})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Optionally add a ",(0,i.jsx)(n.code,{children:'"components": ["A", "KU"]'})," array for reference; it will help later corpus studies but is unnecessary for the challenge metric."]}),"\n",(0,i.jsx)(n.h3,{id:"3-proposed-dataset-split",children:"(3) Proposed dataset split"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Split"}),(0,i.jsx)(n.th,{children:"Images"}),(0,i.jsx)(n.th,{children:"Masks"}),(0,i.jsx)(n.th,{children:"Purpose"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Train"}),(0,i.jsx)(n.td,{children:"7,000"}),(0,i.jsx)(n.td,{children:"100 k - 150 k"}),(0,i.jsx)(n.td,{children:"model fitting"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Dev / public-leaderboard"}),(0,i.jsx)(n.td,{children:"1,500"}),(0,i.jsx)(n.td,{children:"15 k - 20 k"}),(0,i.jsx)(n.td,{children:"parameter tuning; visible scores"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Private test"}),(0,i.jsx)(n.td,{children:"1,500"}),(0,i.jsx)(n.td,{children:"15 k - 20 k"}),(0,i.jsx)(n.td,{children:"final ranking; hidden labels"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Keep the original frequency imbalance of signs; do not artificially balance the train set\u2014realistic skew is part of the task."}),"\n",(0,i.jsx)(n.p,{children:"For very rare signs (\u226430 masks), either merge them into an \u201cother\u201d class or label them but exclude from scoring."}),"\n",(0,i.jsx)(n.h3,{id:"4-annotation-conventions-json-coco-style",children:"(4) Annotation conventions (JSON, COCO-style)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-JSON",children:'{\n  "images": [\n    { "id": 42, "file_name": "VAT_123.tif", "width": 2048, "height": 1536 }\n  ],\n  "annotations": [\n    {\n      "id": 1017,\n      "image_id": 42,\n      "category_id": 73,          // sign: DU\n      "segmentation": [ [...polygon...] ],\n      "bbox": [x,y,w,h],\n      "iscrowd": 0\n    },\n    {\n      "id": 1018,\n      "image_id": 42,\n      "category_id": 999,         // cluster / ignored\n      "segmentation": [ [...polygon...] ],\n      "bbox": [x,y,w,h],\n      "iscrowd": 1,\n      "components": ["GU", "\u0160E"]  // optional metadata\n    }\n  ],\n  "categories": [\n    { "id": 73,  "name": "DU" },\n    \u2026,\n    { "id": 999, "name": "cluster" }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"Participants train ordinary instance\u2011segmentation models (Mask R\u2011CNN, YOLOv8\u2011seg, etc.); evaluation computes mAP over"})," ",(0,i.jsx)(n.code,{children:"iscrowd\u202f=\u202f0"})," ",(0,i.jsx)(n.em,{children:"masks."})]}),"\n",(0,i.jsx)(n.h3,{id:"5-quality-of-life-tips",children:"(5) Quality-of-life tips"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Active\u2011learning loop"})," \u2013 label 10\u202f% of photos, train a quick model, use its heat\u2011maps to suggest the next uncertain regions; you halve annotation cost."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tile large photos"})," to 1024\u202f\xd7\u202f1024 crops; it speeds annotators and model throughput."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Version control"})," the annotation set (e.g., via DVC) so test data never leaks."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Publish a validation script"})," that:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["checks mask overlap ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mo,{children:"\u2264"}),(0,i.jsx)(n.mi,{children:"\u03b5"})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\leq\u03b5"})]})})}),(0,i.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.7719em",verticalAlign:"-0.136em"}}),(0,i.jsx)(n.span,{className:"mrel",children:"\u2264"}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"\u03b5"})]})]})]})]}),"\n",(0,i.jsxs)(n.li,{children:["verifies every ",(0,i.jsx)(n.code,{children:"category_id"})," is in ",(0,i.jsx)(n.code,{children:"categories"}),","]}),"\n",(0,i.jsx)(n.li,{children:"flags missing/extra images."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"botton-line",children:"Botton line"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Aim for 60\u202fk\u202f\u2013\u202f150\u202fk well\u2011drawn sign masks"})," to give each of the 200 categories several hundred examples."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Mark inseparable clusters with an ",(0,i.jsx)(n.code,{children:"iscrowd"})," (or ",(0,i.jsx)(n.code,{children:"cluster"}),") flag"]})," so they don\u2019t handicap scoring but still enrich the visual grammar your competitors can learn."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Follow a COCO\u2011style JSON schema"}),"; the community tooling, baselines, and evaluation code will work out of the box."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"With those choices you strike the right balance between scholarly fidelity and a tractable computer\u2011vision benchmark."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function l(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);