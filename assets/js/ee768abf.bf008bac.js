"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2955],{8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var i=n(6540);const a={},o=i.createContext(a);function s(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:t},e.children)}},9353:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tutorial_thaumato","title":"Segmentation - a different approach","description":"<meta","source":"@site/docs/06_tutorial_thaumato.md","sourceDirName":".","slug":"/tutorial_thaumato","permalink":"/deeppast/tutorial_thaumato","draft":false,"unlisted":false,"editUrl":"https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/06_tutorial_thaumato.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Segmentation - a different approach","sidebar_label":"4. Segmentation - a different approach","hide_table_of_contents":true}}');var a=n(4848),o=n(8453);const s={title:"Segmentation - a different approach",sidebar_label:"4. Segmentation - a different approach",hide_table_of_contents:!0},r=void 0,l={},c=[{value:"Background",id:"background",level:3},{value:"Walkthrough",id:"walkthrough",level:3},{value:"Node placement",id:"node-placement",level:3},{value:"Surface detection",id:"surface-detection",level:4},{value:"Point Cloud Extraction",id:"point-cloud-extraction",level:4},{value:"Connectivity establishment",id:"connectivity-establishment",level:3},{value:"Segmentation",id:"segmentation",level:4},{value:"Stitching",id:"stitching",level:4},{value:"Mesh reconstruction",id:"mesh-reconstruction",level:4},{value:"Further steps",id:"further-steps",level:3},{value:"Flattening",id:"flattening",level:4},{value:"Rendering",id:"rendering",level:4},{value:"Per-pixel-map and layers",id:"per-pixel-map-and-layers",level:5},{value:"Additional notes",id:"additional-notes",level:3},{value:"References",id:"references",level:3}];function h(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",section:"section",sup:"sup",...(0,o.R)(),...e.components},{Head:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n,{children:[(0,a.jsx)("html",{"data-theme":"dark"}),(0,a.jsx)("meta",{name:"description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"og:type",content:"website"}),(0,a.jsx)("meta",{property:"og:url",content:"https://scrollprize.org"}),(0,a.jsx)("meta",{property:"og:title",content:"Deep Past Challenge"}),(0,a.jsx)("meta",{property:"og:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"og:image",content:"https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"}),(0,a.jsx)("meta",{property:"twitter:card",content:"summary_large_image"}),(0,a.jsx)("meta",{property:"twitter:url",content:"https://scrollprize.org"}),(0,a.jsx)("meta",{property:"twitter:title",content:"Deep Past Challenge"}),(0,a.jsx)("meta",{property:"twitter:description",content:"A $1,000,000+ machine learning and computer vision competition"}),(0,a.jsx)("meta",{property:"twitter:image",content:"https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"})]}),"\n",(0,a.jsx)(t.h3,{id:"background",children:"Background"}),"\n",(0,a.jsxs)(t.p,{children:["This high-level, code-free tutorial explains the main steps of the ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/ThaumatoAnakalyptor",children:(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})})," pipeline for automated segmentation in full 3D.\nFor a more technical walkthrough of the pipeline, see this ",(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=80GhWxmMjPE",children:"walkthrough video"}),".\nFor an introduction to one of the core problems in this pipeline where we need your help, check out the ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/graph_problem/tree/main",children:"Sheet Stitching Problem Playground"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["Thaumato is one approach but there may be others - for example, volumetric instance segmentation might be useful as an input to later mesh stitching algorithms.\nIf interested, check out ",(0,a.jsx)(t.a,{href:"https://colab.research.google.com/github/ScrollPrize/vesuvius/blob/main/notebooks/example3_cubes_bootstrap.ipynb",children:"this notebook"})," providing easy access to volumetric segmentation labels."]}),"\n",(0,a.jsx)(t.h3,{id:"walkthrough",children:"Walkthrough"}),"\n",(0,a.jsx)(t.p,{children:"We will tackle the task of segmentation from another perspective: that of a Neapolitan fisherman."}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[300px] m-2",src:"/img/tutorials/fisherman.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Neapolitan fisherman."})]}),"\n",(0,a.jsxs)(t.p,{children:["As we saw in the ",(0,a.jsx)(t.a,{href:"/tutorial3",children:'"Segmentation and Flattening" tutorial'}),", the ultimate purpose of segmentation is that of identifying a triangular mesh on the wrapped papyrus sheet."]}),"\n",(0,a.jsx)(t.p,{children:"Imagine being Neapolitan fishermen, casting our nets not into schools of fish... but onto sheets!\nWe want the net to adhere perfectly on the inner part of the sheet, that is next to where ink is supposed to be located. Indeed, the ink lies on the face of the papyrus that looks towards the pole of rotation of the scroll."}),"\n",(0,a.jsxs)(t.p,{children:["Here, adherence means every node of the net should be placed on the inner surface of the sheet, with adjacent nodes also being adjacent on the sheet. Therefore the problem is twofold: ",(0,a.jsx)(t.em,{children:"node placement"})," and ",(0,a.jsx)(t.em,{children:"connectivity establishment"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"But how can we cast the net in? After all, the data we have is a 3D volumetric image composed of colored blocks.\nYou can think of it as a big volume made of single units of grayscale LEGO blocks (Figure 1, left). The color of a block represents the density of the material in that region of space: lighter means more dense, hence could be papyrus, while darker means less dense, and could be air. The ink is somewhere in there."}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsxs)("div",{className:"flex flex-wrap justify-center mx-auto",children:[(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/lego.webp"}),(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/scroll_6_7_21.webp"})]}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 1. Scroll data as LEGO blocks and sagittal view of a subvolume slice."})]}),"\n",(0,a.jsxs)(t.p,{children:["The right image in Figure 1 displays the view of a section of the volumetric image. Imagine cutting the volume in subvolumes. Take one and look at it from above. Every block composing the subvolume (called ",(0,a.jsx)(t.em,{children:"voxels"}),") will look like a colored square. That is the color of the pixel in the image."]}),"\n",(0,a.jsx)(t.h3,{id:"node-placement",children:"Node placement"}),"\n",(0,a.jsxs)(t.p,{children:["To perform ",(0,a.jsx)(t.em,{children:"node placement"})," we have to detect all the blocks composing the inner surface of the sheet, and forget about the rest.\nThis accounts to creating a mask where non-edge blocks are flagged with 0s and edge blocks are flagged with 1s. In layman's terms, color in black all the non-edge voxels, and in white the edge voxels."]}),"\n",(0,a.jsx)(t.h4,{id:"surface-detection",children:"Surface detection"}),"\n",(0,a.jsxs)(t.p,{children:["What ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," does is exactly this: it performs surface detection in 3D by convolving the volumetric image with a 3D ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Sobel_operator",children:(0,a.jsx)(t.code,{children:"Sobel Kernel"})}),".\nIn simpler terms, it computes a discrete approximation of the 3D color gradient. Then, driven by the intuition that the greatest variation in color will be on an edge, it filters out all the voxels for which the magnitude of a gradient is ",(0,a.jsx)(t.em,{children:"small"})," (below a given threshold)."]}),"\n",(0,a.jsxs)(t.p,{children:["At the same time, considering for every voxel a vector that points towards the ",(0,a.jsx)(t.em,{children:"umbilicus"})," of the scroll (the pole of rotation), and taking the dot product between this vector and the gradient computed just before, one can filter out as well all the voxels that are on the face of the sheet that don't look towards the center. In Figure 2 we display a sagittal view of the mask for the subvolume showed in Figure 1. The image was obtained with a modified version of ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"}),"."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[300px]",src:"/img/tutorials/scroll_mask.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 2. Sheet inner surface mask for a subvolume (sagittal view)."})]}),"\n",(0,a.jsx)(t.h4,{id:"point-cloud-extraction",children:"Point Cloud Extraction"}),"\n",(0,a.jsxs)(t.p,{children:["Unfortunately, our net still cannot get in. Instead of having grayscale LEGO blocks, we now have black/white LEGO blocks.\nThe colors of the blocks changed, but we are still talking about a dense volume. In order for the net to get in we need to create some ",(0,a.jsx)(t.em,{children:"empty space"}),".\nThis means performing a change of representation.\nInstead of dealing with LEGO blocks located at positions (i,j,k) on a tridimensional grid, we will transform the blocks into ",(0,a.jsx)(t.em,{children:"infinitesimal points"})," located at positions (x,y,z) in Euclidean space."]}),"\n",(0,a.jsxs)(t.p,{children:["At the same time, we throw away all the black blocks (that now are points), and we keep just the white ones (those that lie on the sheet surface).\nWe extracted what is called a ",(0,a.jsx)(t.em,{children:"point cloud"}),". In Figure 3 we show a point cloud obtained with a modified version of ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"}),", colors are added just for the purpose of display, since the cloud is very dense. As you may notice, the subvolume looks like a jungle of points that kind of align in parallel sheets. Remember to keep a copy of the computed gradients since they will be eventually needed in the next steps."]}),"\n",(0,a.jsxs)(t.p,{children:["Exploiting the created empty space, our fisherman net can now ",(0,a.jsx)(t.em,{children:"enter"})," the scroll. Every node of the fisherman's net will be placed on a point in the point cloud. But how exactly? We want the net to overlay a contiguous ",(0,a.jsx)(t.em,{children:"patch"})," of surface, without connecting parallel sheets. This problem will be addressed in the next section."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[300px]",src:"/img/tutorials/point_cloud.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 3. Point cloud, colors added for visualization's sake."})]}),"\n",(0,a.jsx)(t.h3,{id:"connectivity-establishment",children:"Connectivity establishment"}),"\n",(0,a.jsxs)(t.p,{children:["We now have a cloud of points, and it's a mess! In order to ",(0,a.jsx)(t.em,{children:"fish"})," a single, continguous patch of a sheet, we first have to identify adjacent parts of the sheets, dispatching every point in its proper neighborhood."]}),"\n",(0,a.jsx)(t.h4,{id:"segmentation",children:"Segmentation"}),"\n",(0,a.jsx)(t.p,{children:"We don't want to select points that lie on parallel surfaces, since this would mean to identify as contiguous points that are not really contiguous in the scroll! We diplay the nature of this problem in the left part of Figure 4, where correctly grouped points are circled in red, and incorrectly grouped points are circled in purple."}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsxs)("div",{className:"flex flex-wrap justify-center mx-auto",children:[(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/scroll_mask_groups.webp"}),(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/sheets.webp"})]}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 4. Identifying patches. (Left) Correct and incorrect grouping; (right) identified patches colored in the point cloud."})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," performs this operation with ",(0,a.jsx)(t.code,{children:"Mask3D"})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-2",id:"user-content-fnref-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"2"})}),", a Deep Neural Network specifically trained for the purpose.\nThe results should look somewhat like the right part of Figure 4 (that represents a different subvolume, less densely packed). The algorithm segments point clouds in contiguous patches of sheet."]}),"\n",(0,a.jsx)(t.p,{children:"Unfortunately, we have to face two unexpected problems:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["the algorithm creates patches with ",(0,a.jsx)(t.em,{children:"holes"})," and in the holes there are other smaller patches;"]}),"\n",(0,a.jsx)(t.li,{children:"the area of these patches is too small."}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," tackles this issue with ",(0,a.jsx)(t.em,{children:"stitching"}),"."]}),"\n",(0,a.jsx)(t.h4,{id:"stitching",children:"Stitching"}),"\n",(0,a.jsx)(t.p,{children:"Stitching is an extension of the point cloud segmentation problem addressed in the previous step, but on patches rather than points."}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[250px]",src:"/img/tutorials/stitching.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 5. Stitching adjacent patches (with the same winding number) together."})]}),"\n",(0,a.jsxs)(t.p,{children:["In Figure 5 different patches have different colors and patches that are stitched together are linked with red arcs. In the end, all the patches that are stitched together will form a segment, hopefully big enough to read on it some columns of text with ink detection models. It is worth saying that the segmentation performed by ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," generates ",(0,a.jsx)(t.em,{children:"overlapping"})," patches. The patch overlap is exploited during the stitching process.\n",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," performs stitching in a sort of Monte Carlo fashion. It first builds ",(0,a.jsx)(t.em,{children:"an uncertainty graph"})," where nodes of this graph are patches and edges are weighted by the amount of overlap between patches. Many random walks are launched of the graph to build a subgraph (to select a cover of nodes/patches). The covers that selected the edges with the maximal overlap are chosen as final segments."]}),"\n",(0,a.jsx)(t.h4,{id:"mesh-reconstruction",children:"Mesh reconstruction"}),"\n",(0,a.jsxs)(t.p,{children:["Now that we have a few big patches, represented as groups of point clouds made of contiguous points on the surface of the sheet, we can select one, and perform ",(0,a.jsx)(t.em,{children:"mesh reconstruction"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"This means finally to establish a connectivity relationship between points: to decide which nodes that are connected in the fisherman net are connected in the point cloud as well. In layman's term, we find a way to connect points with edges!"}),"\n",(0,a.jsxs)(t.p,{children:["Surface reconstruction can be performed with several algorithms, such as ",(0,a.jsx)(t.code,{children:"Poisson Surface Reconstruction"})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-3",id:"user-content-fnref-3","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"3"})})," (that leverages both the positions and the normals to the points, i.e. the gradients that we computed in the previous steps) and ",(0,a.jsx)(t.code,{children:"Delaunay's triangulation"})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-4",id:"user-content-fnref-4","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"4"})}),". ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," uses a mixture of both."]}),"\n",(0,a.jsxs)(t.p,{children:["If instead of working on subvolumes we work on the full volume of the scroll, we can envisage ending up with a large contiguous mesh that is wrapped several times around the umbilicus. This is what ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," can do, visualized in Figure 6."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsxs)("div",{className:"flex flex-wrap justify-center mx-auto",children:[(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/mesh_0.webp"}),(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/mesh_1.webp"}),(0,a.jsx)("img",{className:"max-h-[250px] m-2",src:"/img/tutorials/mesh_2.webp"})]}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 6. Large mesh wrapped several times around the center. (Left) Side view; (center) from above; (right) zoomed-in view."})]}),"\n",(0,a.jsx)(t.p,{children:"In Figure 6, the vertices of the mesh are shown in yellow and the edges connecting them in black. It is worth noting from the zoomed-in view that nearby points are connected by edges and form a triangular mesh."}),"\n",(0,a.jsx)(t.p,{children:"Congratulations! You automatically fished the scroll, or a subpart of it!"}),"\n",(0,a.jsx)(t.h3,{id:"further-steps",children:"Further steps"}),"\n",(0,a.jsxs)(t.p,{children:["In this extra section we describe how from a triangular mesh we can obtain the 2D image of a sheet. This process involves two main steps: ",(0,a.jsx)(t.em,{children:"flattening"})," and ",(0,a.jsx)(t.em,{children:"rendering"}),"."]}),"\n",(0,a.jsx)(t.h4,{id:"flattening",children:"Flattening"}),"\n",(0,a.jsxs)(t.p,{children:["Now that we have a mesh, we must compute a 3D->2D map to obtain a UV (two-dimensional) parametrization for its vertices. This is a fancy way to say that you have to flatten the mesh. After all, our purpose is to eventually read the ink on a ",(0,a.jsx)(t.em,{children:"flat"})," image."]}),"\n",(0,a.jsxs)(t.p,{children:["A community fork ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-5",id:"user-content-fnref-5","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"5"})}),", recently merged to ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"}),", implements an algorithm called ",(0,a.jsx)(t.code,{children:"SLIM"})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-6",id:"user-content-fnref-6","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"6"})})," that aims to find a map that minimizes an ",(0,a.jsx)(t.em,{children:"isometric distortion energy"}),". This means that all the points that are equidistant in 3D will be mapped as equidistant as possible in 2D.\nThe vertices of the mesh shown in Figure 6 are displayed using the obtained UV parametrization in Figure 7."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[500px]",src:"/img/tutorials/flat_mesh.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 7. UV parametrization of the vertices of the mesh displayed in Figure 6."})]}),"\n",(0,a.jsxs)(t.p,{children:["The mesh, along with its UV parametrization, are finally saved in a ",(0,a.jsx)(t.code,{children:".obj"})," file."]}),"\n",(0,a.jsx)(t.h4,{id:"rendering",children:"Rendering"}),"\n",(0,a.jsx)(t.p,{children:'It is extremely important that the obtained UV coordinates are still real numbers. This means that between points there is a lot of empty space!\nIn order to go back to a "pixel-style" representation, we have to convert the plot in Figure 7 to a pixelized image. Notice that in Figure 7 we displayed axes with ticks, this was not a mistake.'}),"\n",(0,a.jsx)(t.h5,{id:"per-pixel-map-and-layers",children:"Per-pixel-map and layers"}),"\n",(0,a.jsxs)(t.p,{children:["For every integer couple (x,y) in the figure we insert a new point. These coordinates (x,y) will be the integers' positions (i,j) of pixels in the rendered image. Eventually, we will end up with a number of points way higher than that of the vertices of the mesh. We will identify in which triangle of the triangular mesh (in 2D) these points fall and compute their ",(0,a.jsx)(t.code,{children:"barycentric coordinates"}),": three coefficients that allow to define the position of every point in a triangle as a linear combination of the positions of the vertices. In Figure 8 we show a red point whose position will be ",(0,a.jsx)(t.code,{children:"0.2 * vertex1 + 0.3 vertex2 + 0.5 vertex3"}),"."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[400px] m-2",src:"/img/tutorials/barycentric.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 8. A red point in a triangle represented with its barycentric coordinates."})]}),"\n",(0,a.jsxs)(t.p,{children:["Exploiting the barycentric coordinates, we can then map back every additional point that we inserted in the UV map to its alleged position in 3D space.\nThis step is called obtaining a ",(0,a.jsx)(t.em,{children:"per-pixel-map"})," and it's performed in ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," using a recently merged community fork ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-7",id:"user-content-fnref-7","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"7"})}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["Why do we need the 3D positions of the new points? Remember that during the first step, ",(0,a.jsx)(t.em,{children:"Node Placement"}),", we totally forgot about the density of the material obtained via the CT scan (its original color) and started working on a black and white mask, that later became a point cloud, and so on. All the colors displayed in Figures 3-7 were only for representation purposes."]}),"\n",(0,a.jsxs)(t.p,{children:["Now we need to recover that information, and to do so we need to know the 3D position of every new point that is going to become a pixel of our rendered image.\nUnfortunately, the map from integer 2D positions to 3D points will result in points with non-integer positions. What does it mean? After all, the original voxels (Figure 1) ",(0,a.jsx)(t.em,{children:"only"}),' have integer positions. What "color" will these new points/pixels then have?']}),"\n",(0,a.jsxs)(t.p,{children:["We are going to compute the color of the new points by ",(0,a.jsx)(t.code,{children:"trilinear interpolation"})," with their closest ones in the volume.\nThis will allow us to obtain images with a smooth varying color. The segment from Figure 7 rendered as a 2D image is displayed in Figure 9."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[500px] m-2",src:"/img/tutorials/thaumato_composite_down.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 9. The segment from Figure 7 rendered as a 2D image."})]}),"\n",(0,a.jsx)(t.p,{children:"We forget to mention that since ink prediction is performed not on a single layer, but in what is called a surface volume, that is a stack of surfaces parallel to the surface we obtained along this pipeline, we need to store and transform as well the values of the normals to the surface at each point.\nWe already computed them for the vertices of the mesh, so we only need to compute the ones for the new points using barycentric coordinates."}),"\n",(0,a.jsxs)(t.p,{children:["In Figure 10 we show the ink prediction from the ",(0,a.jsx)(t.code,{children:"Phase 1 Grand Prize winning model"})," ",(0,a.jsx)(t.sup,{children:(0,a.jsx)(t.a,{href:"#user-content-fn-8",id:"user-content-fnref-8","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"8"})})," on the stack of rendered layers obtained from the segment displayed in Figure 7 - the composite image is shown in Figure 9."]}),"\n",(0,a.jsxs)("figure",{className:"text-center mx-auto",children:[(0,a.jsx)("div",{className:"flex flex-wrap justify-center mx-auto",children:(0,a.jsx)("img",{className:"max-h-[500px] m-2",src:"/img/tutorials/thaumato_ink_detection.webp"})}),(0,a.jsx)("figcaption",{className:"mt-1",children:"Figure 10. Ink prediction on the layers rendered from the UV in Figure 7."})]}),"\n",(0,a.jsx)(t.h3,{id:"additional-notes",children:"Additional notes"}),"\n",(0,a.jsxs)(t.p,{children:["Please notice that every single one of the previously mentioned steps can be improved, both in terms of mesh generation quality and computational efficiency.\nFor further information on ",(0,a.jsx)(t.code,{children:"Thaumato Anakalyptor"})," and its roadmap, please read its official ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/ThaumatoAnakalyptor/blob/main/documentation/ThaumatoAnakalyptor___Technical_Report_and_Roadmap.pdf",children:"Technical Report"}),"."]}),"\n",(0,a.jsx)(t.h3,{id:"references",children:"References"}),"\n","\n",(0,a.jsxs)(t.section,{"data-footnotes":!0,className:"footnotes",children:[(0,a.jsx)(t.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{id:"user-content-fn-1",children:["\n",(0,a.jsxs)(t.p,{children:["Schilliger et al. (2023). ",(0,a.jsx)(t.em,{children:"Thaumato Anakalyptor"}),". Retrieved from ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/ThaumatoAnakalyptor",children:"https://github.com/schillij95/ThaumatoAnakalyptor"})," ",(0,a.jsx)(t.a,{href:"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-2",children:["\n",(0,a.jsxs)(t.p,{children:["Schult et al. (2023). ",(0,a.jsx)(t.em,{children:"Mask3D: Mask Transformer for 3D Semantic Instance Segmentation"}),". Paper presented at the International Conference on Robotics and Automation (ICRA). ",(0,a.jsx)(t.a,{href:"#user-content-fnref-2","data-footnote-backref":"","aria-label":"Back to reference 2",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-3",children:["\n",(0,a.jsxs)(t.p,{children:["Kazhdan et al. (2006). ",(0,a.jsx)(t.em,{children:"Poisson Surface Reconstruction"}),". In Proceedings of the Eurographics Symposium on Geometry Processing. ",(0,a.jsx)(t.a,{href:"#user-content-fnref-3","data-footnote-backref":"","aria-label":"Back to reference 3",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-4",children:["\n",(0,a.jsxs)(t.p,{children:["Delaunay, B. (1934). ",(0,a.jsx)(t.em,{children:"Sur la sph\xe8re vide"}),". Bulletin de l'Acad\xe9mie des Sciences de l'URSS, Classe des Sciences Math\xe9matiques et Naturelles, 6, 793-800. ",(0,a.jsx)(t.a,{href:"#user-content-fnref-4","data-footnote-backref":"","aria-label":"Back to reference 4",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-5",children:["\n",(0,a.jsxs)(t.p,{children:["Angelotti, G. (2024). ",(0,a.jsx)(t.em,{children:"slim-flatboi"}),". Retrieved from ",(0,a.jsx)(t.a,{href:"https://github.com/giorgioangel/slim-flatboi",children:"https://github.com/giorgioangel/slim-flatboi"})," and ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/ThaumatoAnakalyptor/pull/2",children:"https://github.com/schillij95/ThaumatoAnakalyptor/pull/2"})," ",(0,a.jsx)(t.a,{href:"#user-content-fnref-5","data-footnote-backref":"","aria-label":"Back to reference 5",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-6",children:["\n",(0,a.jsxs)(t.p,{children:["Rabinovich et al. (2017). ",(0,a.jsx)(t.em,{children:"Scalable Locally Injective Mappings"}),". ACM Transactions on Graphics (TOG), 36(4), 1. ",(0,a.jsx)(t.a,{href:"#user-content-fnref-6","data-footnote-backref":"","aria-label":"Back to reference 6",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-7",children:["\n",(0,a.jsxs)(t.p,{children:["Angelotti, G. (2024). ",(0,a.jsx)(t.em,{children:"Democratizing rendering"}),". Retrieved from ",(0,a.jsx)(t.a,{href:"https://github.com/schillij95/ThaumatoAnakalyptor/pull/6",children:"https://github.com/schillij95/ThaumatoAnakalyptor/pull/6"})," ",(0,a.jsx)(t.a,{href:"#user-content-fnref-7","data-footnote-backref":"","aria-label":"Back to reference 7",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{id:"user-content-fn-8",children:["\n",(0,a.jsxs)(t.p,{children:["Nader et al. (2023). ",(0,a.jsx)(t.em,{children:"Vesuvius Grandprize Winning Solution"}),". Retrieved from ",(0,a.jsx)(t.a,{href:"https://github.com/younader/Vesuvius-Grandprize-Winner",children:"https://github.com/younader/Vesuvius-Grandprize-Winner"})," ",(0,a.jsx)(t.a,{href:"#user-content-fnref-8","data-footnote-backref":"","aria-label":"Back to reference 8",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);