# YOU DO NOT HAVE TO PROVIDE VALUES IN MOST OF THESE FIELDS

tr_setup:
  model_name: Default_Config                    # required [str] this is the name your model will checkpoint will be saved as
  autoconfigure: true               # optional, true -- if true, the network will attempt to set some reasonable defaults based on your vram_max
  tr_val_split: 0.80                 # optional[float] the percentage of your total dataset used for training, with the other part being used for val
  dilate_label: false                # optional, false [bool] if true, will apply a small dilation to your labels using a 3x3 spherical kernel -- will skip tasks named "normals"
  # checkpoint_path: "./ink_157.pth" # optional, none [str] if provided, will load the provided checkpoint and begin training from there
  load_weights_only: false           # optional, false [bool] if true, will not load the optimizer, scheduler, or epoch state from the model -- set true to fine-tune
  ckpt_out_base: "./checkpoints"     # [str] the path the model checkpoint is saved to

tr_config:
  optimizer: "AdamW"            # optional, AdamW [str] the optimizer to use during training. currently only AdamW and SGD are provided.
  initial_lr: 0.0001             # optional initial learning rate
  weight_decay: 0.0001             # optional, 0.0001 [float]
  gradient_accumulation: 1      # optional, 1 [int] if 1, no accumulation, if >1 will accumulate this many 'batches' each batch to simulate larger batch size.
  num_dataloader_workers: 4    # optional, 4 [int]
  patch_size: [96, 96]    # optional [list] patch size for training
  batch_size: 2              # optional [int] batch size for training
  max_steps_per_epoch: 250      # optional, 500 [int] the number of batches seen by the model each epoch
  max_val_steps_per_epoch: 25   # optional, 25 [int] the number of batches seen in validation each epoch
  max_epoch: 15                # optional, 500 [int] the maximum number of epochs to train for
  ignore_label: null            # [NOT YET IMPLEMENTED] if you have an ignore label, you can set it here and loss will not be computed against it
  loss_only_on_label: false     # [NOT YET IMPLEMENTED]

model_config:
  use_timm_encoder: false
  timm_encoder_class: null

dataset_config:
  min_bbox_percent: 0.30      # optional, 0.97 [float] a percentage of the patch size that must be encompassed by a bbox containing all the labels in the patch
  min_labeled_ratio: 0.25     # optional, 0.15 [float] a percentage of the above bbox that must contain labeled data (ie: the density of the labels)

inference_config:                           # these parameters only apply on inference
  checkpoint_path: "./checkpoints/ckpt.pth" # the checkpoint to load
  num_dataloader_workers: 4                # number of workers for the inference dataloader
  overlap: 0.5                              # optional, .01 - the percentage of overlap each patch should have
  batch_size: 4                             # can typically be higher than your train batch size, watch vram
  targets:                                  # must be the targets you trained with , including num channels and name.
    - ink:                                    # to the number of channels your model outputs
        channels: 1
        activation: "none"
        weight: 1
