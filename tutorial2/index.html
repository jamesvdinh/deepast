<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-tutorial2" data-theme="dark" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Tutorial: Representation | Deep Past Challenge</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:type" content="website"><meta data-rh="true" property="og:url" content="https://scrollprize.org"><meta data-rh="true" property="og:title" content="Vesuvius Challenge"><meta data-rh="true" property="og:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:image" content="https://scrollprize.org/img/social/opengraph.jpg"><meta data-rh="true" property="twitter:card" content="summary_large_image"><meta data-rh="true" property="twitter:url" content="https://scrollprize.org"><meta data-rh="true" property="twitter:title" content="Vesuvius Challenge"><meta data-rh="true" property="twitter:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="twitter:image" content="https://scrollprize.org/img/social/opengraph.jpg"><link data-rh="true" rel="icon" href="/deepast/img/social/favicon.ico"><link data-rh="true" rel="canonical" href="https://jamesvdinh.github.io/deepast/tutorial2"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/tutorial2" hreflang="en"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/tutorial2" hreflang="x-default"><script src="https://cdn.usefathom.com/script.js" data-site="XERDEBQR" defer="defer" data-spa="auto"></script><link rel="stylesheet" href="/deepast/assets/css/styles.8537f981.css">
<script src="/deepast/assets/js/runtime~main.962d1e25.js" defer="defer"></script>
<script src="/deepast/assets/js/main.634575ab.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/deepast/"><div class="navbar__logo"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Past Challenge</b></a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="docItemContainer_Djhp"><article><div class="theme-doc-markdown markdown"><header><h1>Tutorial: Representation</h1></header>
<!-- -->
<div class="mx-[-16px] sm:mx-0 flex flex-wrap items-start mb-4 text-center justify-center sm:justify-start"><a href="/tutorial1" class="mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl bg-[#fffefc30] hover:bg-[#fefefe45]"><video autoplay="" playsinline="" loop="" muted="" class="w-[100%] rounded-xl mb-2" poster="/img/tutorial-thumbs/top-scanning-small.webp"><source src="/img/tutorial-thumbs/top-scanning-small.webm" type="video/webm"></video><div class="text-sm">Scanning</div></a><div class="hidden sm:flex mx-2 mb-2 flex-col items-center"><div class="relative leading-[150px] py-4 w-[16px] text-center">→</div><div class="text-sm"> </div></div><a href="/tutorial3" class="mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl"><video autoplay="" playsinline="" loop="" muted="" class="w-[100%] rounded-xl mb-2" poster="/img/tutorial-thumbs/top-segmentation-small.webp"><source src="/img/tutorial-thumbs/top-segmentation-small.webm" type="video/webm"></video><div class="text-sm">Segmentation and Flattening</div></a><div class="hidden sm:flex mx-2 mb-2 flex-col items-center"><div class="relative leading-[150px] py-4 w-[16px] text-center">→</div><div class="text-sm"> </div></div><a href="/tutorial4" class="mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl"><img class="max-h-[300px] m-2" src="/img/tutorials/fisherman.webp"><div class="text-sm">Segmentation - a different approach</div></a><div class="hidden sm:flex mx-2 mb-2 flex-col items-center"><div class="relative leading-[150px] py-4 w-[16px] text-center">→</div><div class="text-sm"> </div></div><a href="/tutorial5" class="mb-2 flex flex-col items-center w-[100px] sm:w-[150px] relative box-content p-2 sm:p-4 sm:pb-2 hover:bg-[#fefefe26] rounded-xl"><video autoplay="" playsinline="" loop="" muted="" class="w-[100%] rounded-xl mb-2" poster="/img/tutorial-thumbs/top-prediction-small3.webp"><source src="/img/tutorial-thumbs/top-prediction-small.webm" type="video/webm"></video><div class="text-sm">Ink Detection</div></a></div>
<p>The micro-CT scan converts the scroll into a volumetric grid of density measurements, providing a mathematical description with integer coordinates and average material densities in small adjacent cubes known as voxels.</p>
<div class="flex max-w-[400px]"><div class="w-[75%] mb-2 mr-2"><img src="/img/tutorials/rep_ash2vox.webp" class="w-[100%]"><figcaption class="mt-0">Physical scroll to voxel representation</figcaption></div></div>
<p>While the raw scan volume represents the maximum amount of information available, this dataset is huge and the papyrus surface is difficult to locate throughout the scroll. Mathematical calculations and machine learning can be used to manipulate the digital dataset into different representations that are easier to interpret both visually and computationally.</p>
<p>Besides the raw scan volume, three types of representations have been explored:</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Machine Learning Segmentation</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Traditional voxel processing</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Point Clouds</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="bg-gray-800 p-4 rounded mb-4"><p><strong>Input:</strong> 3D scan volume (voxels in .tif “image stack”)<br>
<strong>Output:</strong> binary 3D volume (voxels in .tif “image stack”)</p></div><p>Machine learning segmentation involves separating the part of the papyrus we wish to work with from the rest of the scan. We have trained models on a number of different papyrus features, but our current methods focus on two structures that exist within the scroll: The recto (inside) or verso (outside) surfaces, and the fibers which make up the papyrus sheet itself.</p><p>In the image below, the purple lines mark the recto surface, giving us a much more tractable representation to work with on downstream tasks than the scan itself.</p><div class="flex max-w-[400px]"><div class="w-[75%] mb-2 mr-2"><img src="/img/tutorials/s1_surface_pred.png" class="w-[100%]"><figcaption class="mt-0">Semantic segmentation of the recto surface</figcaption></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><h3></h3><div class="bg-gray-800 p-4 rounded mb-4"><p><strong>Input:</strong> 3D scan volume (voxels in .tif “image stack”)<br>
<strong>Output:</strong> new 3D volume (voxels in .tif “image stack”)</p></div><p>Traditional image processing techniques have mostly been intended to aid manual segmentation, either to make the papyrus easier to interpret or to help segmentation algorithms follow the surface better. Popular techniques include thresholding, noise reduction, various colourization schemes, edge detection, and skeletonization.</p><div class="flex max-w-[400px]"><div class="w-[75%] mb-2 mr-2"><img src="/img/tutorials/rep_trad_voxel.webp" class="w-[100%]"><figcaption class="mt-0">Voxel to voxel representations using traditional techniques.</figcaption></div></div><p>Most of these techniques have not been successful at improving segmentation.</p></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="bg-gray-800 p-4 rounded mb-4"><p><strong>Input:</strong> 3D scan volume (voxels in .tif “image stack”)<br>
<strong>Output:</strong> point cloud (.ply or .obj)</p></div><p>The goal of point clouds is to extract a limited but useful subset of information from the raw scan volume. In the point cloud representation, calculations select voxels that describe the surface of a sheet.</p><div class="flex max-w-[400px]"><div class="w-[75%] mb-2 mr-2"><img src="/img/tutorials/rep_vox2pointcloud.webp" class="w-[100%]"><figcaption class="mt-0">Transforming voxels into a point cloud representation.</figcaption></div></div><p>The fixed grid and density values are discarded, using traditional edge/surface detection to generate infinitesimal points characterized by 3D real coordinates. If needed, the discarded information can be reattached to the points to revert to the voxel representation.</p></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="semantic-segmentation-of-scroll-structures">Semantic Segmentation of Scroll Structures<a href="#semantic-segmentation-of-scroll-structures" class="hash-link" aria-label="Direct link to Semantic Segmentation of Scroll Structures" title="Direct link to Semantic Segmentation of Scroll Structures">​</a></h2>
<p>This guide will focus on utilizing machine learning to create semantic segmentations of recto surfaces, but the same methods apply to any of our other datasets, such as fibers -- simply swap out the training images and their associated labels. It is written using Ubuntu 22.04, but the guide should translate to most any version of Linux. Windows users are advised to use WSL2 and place their data within the Linux filesystem.</p>
<p>For this tutorial, we&#x27;ll be using a library/framework known as <a href="https://github.com/MIC-DKFZ/nnUNet" target="_blank" rel="noopener noreferrer">nnUNetv2</a></p>
<p>nnUNet is a well regarded and highly performant machine learning library with a focus on volumetric data within the medical domain, which has many parallels to our imaging modalities (namely xray tomography), and provides us with a fantastic baseline and starting point for experimentation.</p>
<p>This is a simplified guide, it is highly recommended to visit <a href="https://github.com/MIC-DKFZ/nnUNet" target="_blank" rel="noopener noreferrer">the repository</a>and read further after following this!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="installing-nnunet-and-its-dependencies">Installing nnUNet and its Dependencies<a href="#installing-nnunet-and-its-dependencies" class="hash-link" aria-label="Direct link to Installing nnUNet and its Dependencies" title="Direct link to Installing nnUNet and its Dependencies">​</a></h3>
<p><strong>Requirements</strong></p>
<ul>
<li>Working CUDA installation -- <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">Installation instructions</a></li>
<li>Nvidia GPU with 10GB VRAM</li>
<li>32GB CPU RAM (although you may be able to get by with a bit less)</li>
</ul>
<p>If you do not already have a virtual environment manager, install miniconda:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">source $HOME/miniconda3/bin/activate</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Create a virtual environment for nnUNet:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">conda create -n nnunet python=3.10</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">conda activate nnunet</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Install Pip within the Conda Environment (this is done because PyTorch no longer creates Conda packages)</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">conda install pip </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Install PyTorch</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install torch torchvision</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Install nnUNetv2. if you wish to extend nnUNet, you can install in editable mode by following <a href="https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/installation_instructions.md#installation-instructions" target="_blank" rel="noopener noreferrer">these instructions</a></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install nnunetv2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-preparation-and-preprocessing">Data Preparation and Preprocessing<a href="#data-preparation-and-preprocessing" class="hash-link" aria-label="Direct link to Data Preparation and Preprocessing" title="Direct link to Data Preparation and Preprocessing">​</a></h3>
<p>Download and extract the <a href="https://dl.ash2txt.org/community-uploads/bruniss/nnunet_models/nnUNet_raw/Dataset060_s1_s4_s5_patches_frangiedt_test.zip" target="_blank" rel="noopener noreferrer">test dataset</a>. Choose a folder you&#x27;d like to store your nnUNet data in, and structure the folders like so, placing the dataset where indicated in the tree:</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── nnUNet_data/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── nnUNet_raw/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │   └── Dataset060_s1_s4_s5_patches_frangiedt_test/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │       ├── imagesTr</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │       ├── labelsTr</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │       └── dataset.json</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── nnUNet_preprocessed</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── nnUNet_results</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You may notice by now that the images contain a suffix of _0000 while the labels do not. This suffix maps the images to a normalization scheme defined in <code>dataset.json</code>, for more information if desired, visit <a href="https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/explanation_normalization.md" target="_blank" rel="noopener noreferrer">this page</a>.</p>
<p>nnUNet requires some information about where you have stored this dataset, which it accesses from environment variables. Set them for the current session with the following commands. You can make them permanent if you&#x27;d like by adding the same lines to your <code>~/.bashrc</code> file.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export nnUNet_raw=&quot;/path/to/nnUNet_data/nnUNet_raw/&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export nnUNet_preprocessed=&quot;/path/to/nnUNet_data/nnUNet_preprocessed/&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export nnUNet_results=&quot;/path/to/nnUNet_data/nnUNet_results/&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Now we can begin the preprocessing stage. nnUNet will gather some information about your dataset, verify there is nothing wrong with it, and extract it into uncompressed numpy arrays.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">nnUNetv2_plan_and_preprocess -d 060 -pl nnUNetPlannerResEncM </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This can take a few minutes, and will print some information as it goes. It should finish without errors.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h3>
<p>With our data preprocessed we can finally begin training. The command here will begin a training run using dataset <code>060</code> (the identifier for our test dataset), using the <code>3d_fullres</code> configuration, and fold <code>0</code>.</p>
<p>nnUNet automatically creates 5 training folds for cross-validation on your dataset, and these splits can be modified in the <code>splits_final.json</code> which is generated after initiating training. You can run without any cross-validation by specifying <code>all</code> instead of a fold number.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">nnUNetv2_train 060 3d_fullres 0 -p nnUNetResEncUNetMPlans</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>nnUNet will extract the dataset, and begin training at epoch 0. While training runs it will print the progress in the terminal. The EMA pseudo dice is computed on a random validation batch, and shouldn&#x27;t be taken as gospel, but can provide a relative idea of how well the training is going, provided you have not used the <code>all</code> fold.</p>
<div class="flex max-w-[400px]"><div class="w-[100%] mb-2 mr-2"><img src="/img/tutorials/nnunet_training.png" class="w-[100%]"><figcaption class="mt-0">Training progress</figcaption></div></div>
<p>nnUNet will also store a graph that can be used to assess the current training run located in <code>/path/to/nnUNet_data/nnUNet_results/Dataset060_s1_s4_s5_patches_frangiedt_test/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres/fold_0/progress.png</code></p>
<p>This contains information about current learning rate and epoch time, but of particular interest is the top graph containing information about current psuedo dice and training and validation loss. Ideally, your bar should look like this -- where both training and validation loss are continuing to drop along similar paths.</p>
<p>Towards the end, you can see my training loss continues to drop while my validation loss levels out, this tells us that the model is no longer improving on the validation data, and is likely beginning to overfit to the training data. This is not necessarily bad, but you would not want these lines to diverge too greatly. You can end training at any time by pressing <code>ctrl+c</code> in the terminal window, or allow training to continue for the default <code>max_epochs</code>, which for nnUNet is 1,000.</p>
<div class="flex max-w-[400px]"><div class="w-[100%] mb-2 mr-2"><img src="/img/tutorials/nnunet_progress.png" class="w-[100%]"><figcaption class="mt-0">Training progress</figcaption></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="inference">Inference<a href="#inference" class="hash-link" aria-label="Direct link to Inference" title="Direct link to Inference">​</a></h3>
<p>To run inference with your newly trained model, you first need to ensure that the file names contain the same suffix as the ones you trained with. In our case, this is <code>_0000</code>. <strong>This example will run inference on all of P.Herc. 1667 to show the process of reconstructing the inference back to the original volume, you can simply download a single grid and run inference on only one if you only wish to see what the inference output looks like</strong>!</p>
<p>Download the <a href="https://dl.ash2txt.org/community-uploads/bruniss/scrolls/s4/grids/" target="_blank" rel="noopener noreferrer">Scroll 4 grids</a>. These are 600x600x600 volumes, with the same naming convention as the original scroll grids, but with 50 voxels of overlap in all directions so that when we reconstruct it we can do so in a way that mitigates edge artifacts. Inference on this entire volume takes around 13 hours on two RTX 3090s.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>If you stopped your training early, you&#x27;ll need to go into the results folder in your nnUNet_results path, and change one of the checkpoints to <code>checkpoint_final.pth</code></p></div></div>
<p>Place the data in any folder, and run the command</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">nnUNetv2_predict -d 060 -f 0 -c 3d_fullres -i /path/to/input/grids -o /path/to/output/grids -p nnUNetResEncUNetMPlans</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>for multi-gpu setups, you can run inference like so, repeating for as many gpus as you have available</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CUDA_VISIBLE_DEVICES=0 nnUNetv2_predict d 060 -f 0 -c 3d_fullres -i /path/to/input/folder -o /path/to/output/folder -p nnUNetResEncUNetMPlans -num_parts 2 -part_id 0  &amp;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">CUDA_VISIBLE_DEVICES=1 nnUNetv2_predict d 060 -f 0 -c 3d_fullres -i /path/to/input/folder -o /path/to/output/folder -p nnUNetResEncUNetMPlans -num_parts 2 -part_id 1 </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>To export the probabilities, append --save_probabilities to the end of the inference commands. This will save an npz and a pkl file, which you can turn into uint8 tifs with <a href="https://dl.ash2txt.org/community-uploads/bruniss/scrolls/helper-scripts/mp-softmax.py" target="_blank" rel="noopener noreferrer">this script</a></p></div></div>
<p>Once inference has completed, download <a href="https://dl.ash2txt.org/community-uploads/bruniss/scrolls/helper-scripts/grids_to_zarr.py" target="_blank" rel="noopener noreferrer">this script</a>, and in the main block, change <code>map=False</code> to <code>map=True</code>, and change <code>Scroll=&#x27;1&#x27;</code> to <code>Scroll=&#x27;4&#x27;</code>. This will modify the pixel values of your inference (which are simply 0 for background and 1 for foreground) to 0 and 255, and map your grids to the proper volume shape. You can modify <code>max_workers</code> and <code>chunk_size</code> if desired.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python grids_to_zarr.py</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This script will add your blocks to a zarr array of the proper shape, and trim the 50 voxel overlaps we added previously. The result of this is a single resolution zarr. It is recommended to create a multi-resolution ome-zarr , by using <a href="https://github.com/KhartesViewer/scroll2zarr/blob/main/zarr_to_ome.py" target="_blank" rel="noopener noreferrer">this script</a>, written by Discord member @khartes_chuck.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python zarr_to_ome.py /path/to/single_resolution.zarr /path/to/multi_resolution_ome.zarr --algorithm max</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The final result is the predicted recto surface for the entirety of Scroll 4 that should be much easier to work with than the original scan data in downstream tasks. Volumes like this one are the inputs for methods used in our <a href="/deepast/segmentation">next guide</a> on segmentation.</p>
<div class="flex max-w-[400px]"><div class="w-[100%] mb-2 mr-2"><img src="img/virtual-unwrapping/surface-predictions.jpg" class="w-[100%]"><figcaption class="mt-0">A slice of a predicted surface volume</figcaption></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/05_tutorial2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Overview</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/deepast/get_started">Getting Started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Past Challenge.</div></div></div></footer></div>
</body>
</html>