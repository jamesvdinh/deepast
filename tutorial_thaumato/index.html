<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-tutorial_thaumato" data-theme="dark" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Segmentation - a different approach | Deep Past Challenge</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:type" content="website"><meta data-rh="true" property="og:url" content="https://scrollprize.org"><meta data-rh="true" property="og:title" content="Vesuvius Challenge"><meta data-rh="true" property="og:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="og:image" content="https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"><meta data-rh="true" property="twitter:card" content="summary_large_image"><meta data-rh="true" property="twitter:url" content="https://scrollprize.org"><meta data-rh="true" property="twitter:title" content="Vesuvius Challenge"><meta data-rh="true" property="twitter:description" content="A $1,000,000+ machine learning and computer vision competition"><meta data-rh="true" property="twitter:image" content="https://scrollprize.org/img/social/opengraph.jpg?2024-02-27"><link data-rh="true" rel="icon" href="/deepast/img/social/favicon.ico"><link data-rh="true" rel="canonical" href="https://jamesvdinh.github.io/deepast/tutorial_thaumato"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/tutorial_thaumato" hreflang="en"><link data-rh="true" rel="alternate" href="https://jamesvdinh.github.io/deepast/tutorial_thaumato" hreflang="x-default"><script src="https://cdn.usefathom.com/script.js" data-site="XERDEBQR" defer="defer" data-spa="auto"></script><link rel="stylesheet" href="/deepast/assets/css/styles.8537f981.css">
<script src="/deepast/assets/js/runtime~main.962d1e25.js" defer="defer"></script>
<script src="/deepast/assets/js/main.634575ab.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/deepast/"><div class="navbar__logo"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/deepast/img/social/favicon-64x64.png" alt="Vesuvius Challenge Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Deep Past Challenge</b></a></div><div class="navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="docItemContainer_Djhp"><article><div class="theme-doc-markdown markdown"><header><h1>Segmentation - a different approach</h1></header>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="background">Background<a href="#background" class="hash-link" aria-label="Direct link to Background" title="Direct link to Background">​</a></h3>
<p>This high-level, code-free tutorial explains the main steps of the <a href="https://github.com/schillij95/ThaumatoAnakalyptor" target="_blank" rel="noopener noreferrer"><code>Thaumato Anakalyptor</code></a> <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> pipeline for automated segmentation in full 3D.
For a more technical walkthrough of the pipeline, see this <a href="https://www.youtube.com/watch?v=80GhWxmMjPE" target="_blank" rel="noopener noreferrer">walkthrough video</a>.
For an introduction to one of the core problems in this pipeline where we need your help, check out the <a href="https://github.com/schillij95/graph_problem/tree/main" target="_blank" rel="noopener noreferrer">Sheet Stitching Problem Playground</a>.</p>
<p>Thaumato is one approach but there may be others - for example, volumetric instance segmentation might be useful as an input to later mesh stitching algorithms.
If interested, check out <a href="https://colab.research.google.com/github/ScrollPrize/vesuvius/blob/main/notebooks/example3_cubes_bootstrap.ipynb" target="_blank" rel="noopener noreferrer">this notebook</a> providing easy access to volumetric segmentation labels.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="walkthrough">Walkthrough<a href="#walkthrough" class="hash-link" aria-label="Direct link to Walkthrough" title="Direct link to Walkthrough">​</a></h3>
<p>We will tackle the task of segmentation from another perspective: that of a Neapolitan fisherman.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[300px] m-2" src="/img/tutorials/fisherman.webp"></div><figcaption class="mt-1">Neapolitan fisherman.</figcaption></figure>
<p>As we saw in the <a href="/deepast/tutorial3">&quot;Segmentation and Flattening&quot; tutorial</a>, the ultimate purpose of segmentation is that of identifying a triangular mesh on the wrapped papyrus sheet.</p>
<p>Imagine being Neapolitan fishermen, casting our nets not into schools of fish... but onto sheets!
We want the net to adhere perfectly on the inner part of the sheet, that is next to where ink is supposed to be located. Indeed, the ink lies on the face of the papyrus that looks towards the pole of rotation of the scroll.</p>
<p>Here, adherence means every node of the net should be placed on the inner surface of the sheet, with adjacent nodes also being adjacent on the sheet. Therefore the problem is twofold: <em>node placement</em> and <em>connectivity establishment</em>.</p>
<p>But how can we cast the net in? After all, the data we have is a 3D volumetric image composed of colored blocks.
You can think of it as a big volume made of single units of grayscale LEGO blocks (Figure 1, left). The color of a block represents the density of the material in that region of space: lighter means more dense, hence could be papyrus, while darker means less dense, and could be air. The ink is somewhere in there.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[250px] m-2" src="/img/tutorials/lego.webp"><img class="max-h-[250px] m-2" src="/img/tutorials/scroll_6_7_21.webp"></div><figcaption class="mt-1">Figure 1. Scroll data as LEGO blocks and sagittal view of a subvolume slice.</figcaption></figure>
<p>The right image in Figure 1 displays the view of a section of the volumetric image. Imagine cutting the volume in subvolumes. Take one and look at it from above. Every block composing the subvolume (called <em>voxels</em>) will look like a colored square. That is the color of the pixel in the image.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="node-placement">Node placement<a href="#node-placement" class="hash-link" aria-label="Direct link to Node placement" title="Direct link to Node placement">​</a></h3>
<p>To perform <em>node placement</em> we have to detect all the blocks composing the inner surface of the sheet, and forget about the rest.
This accounts to creating a mask where non-edge blocks are flagged with 0s and edge blocks are flagged with 1s. In layman&#x27;s terms, color in black all the non-edge voxels, and in white the edge voxels.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="surface-detection">Surface detection<a href="#surface-detection" class="hash-link" aria-label="Direct link to Surface detection" title="Direct link to Surface detection">​</a></h4>
<p>What <code>Thaumato Anakalyptor</code> does is exactly this: it performs surface detection in 3D by convolving the volumetric image with a 3D <a href="https://en.wikipedia.org/wiki/Sobel_operator" target="_blank" rel="noopener noreferrer"><code>Sobel Kernel</code></a>.
In simpler terms, it computes a discrete approximation of the 3D color gradient. Then, driven by the intuition that the greatest variation in color will be on an edge, it filters out all the voxels for which the magnitude of a gradient is <em>small</em> (below a given threshold).</p>
<p>At the same time, considering for every voxel a vector that points towards the <em>umbilicus</em> of the scroll (the pole of rotation), and taking the dot product between this vector and the gradient computed just before, one can filter out as well all the voxels that are on the face of the sheet that don&#x27;t look towards the center. In Figure 2 we display a sagittal view of the mask for the subvolume showed in Figure 1. The image was obtained with a modified version of <code>Thaumato Anakalyptor</code>.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[300px]" src="/img/tutorials/scroll_mask.webp"></div><figcaption class="mt-1">Figure 2. Sheet inner surface mask for a subvolume (sagittal view).</figcaption></figure>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="point-cloud-extraction">Point Cloud Extraction<a href="#point-cloud-extraction" class="hash-link" aria-label="Direct link to Point Cloud Extraction" title="Direct link to Point Cloud Extraction">​</a></h4>
<p>Unfortunately, our net still cannot get in. Instead of having grayscale LEGO blocks, we now have black/white LEGO blocks.
The colors of the blocks changed, but we are still talking about a dense volume. In order for the net to get in we need to create some <em>empty space</em>.
This means performing a change of representation.
Instead of dealing with LEGO blocks located at positions (i,j,k) on a tridimensional grid, we will transform the blocks into <em>infinitesimal points</em> located at positions (x,y,z) in Euclidean space.</p>
<p>At the same time, we throw away all the black blocks (that now are points), and we keep just the white ones (those that lie on the sheet surface).
We extracted what is called a <em>point cloud</em>. In Figure 3 we show a point cloud obtained with a modified version of <code>Thaumato Anakalyptor</code>, colors are added just for the purpose of display, since the cloud is very dense. As you may notice, the subvolume looks like a jungle of points that kind of align in parallel sheets. Remember to keep a copy of the computed gradients since they will be eventually needed in the next steps.</p>
<p>Exploiting the created empty space, our fisherman net can now <em>enter</em> the scroll. Every node of the fisherman&#x27;s net will be placed on a point in the point cloud. But how exactly? We want the net to overlay a contiguous <em>patch</em> of surface, without connecting parallel sheets. This problem will be addressed in the next section.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[300px]" src="/img/tutorials/point_cloud.webp"></div><figcaption class="mt-1">Figure 3. Point cloud, colors added for visualization&#x27;s sake.</figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="connectivity-establishment">Connectivity establishment<a href="#connectivity-establishment" class="hash-link" aria-label="Direct link to Connectivity establishment" title="Direct link to Connectivity establishment">​</a></h3>
<p>We now have a cloud of points, and it&#x27;s a mess! In order to <em>fish</em> a single, continguous patch of a sheet, we first have to identify adjacent parts of the sheets, dispatching every point in its proper neighborhood.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="segmentation">Segmentation<a href="#segmentation" class="hash-link" aria-label="Direct link to Segmentation" title="Direct link to Segmentation">​</a></h4>
<p>We don&#x27;t want to select points that lie on parallel surfaces, since this would mean to identify as contiguous points that are not really contiguous in the scroll! We diplay the nature of this problem in the left part of Figure 4, where correctly grouped points are circled in red, and incorrectly grouped points are circled in purple.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[250px] m-2" src="/img/tutorials/scroll_mask_groups.webp"><img class="max-h-[250px] m-2" src="/img/tutorials/sheets.webp"></div><figcaption class="mt-1">Figure 4. Identifying patches. (Left) Correct and incorrect grouping; (right) identified patches colored in the point cloud.</figcaption></figure>
<p><code>Thaumato Anakalyptor</code> performs this operation with <code>Mask3D</code> <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>, a Deep Neural Network specifically trained for the purpose.
The results should look somewhat like the right part of Figure 4 (that represents a different subvolume, less densely packed). The algorithm segments point clouds in contiguous patches of sheet.</p>
<p>Unfortunately, we have to face two unexpected problems:</p>
<ol>
<li>the algorithm creates patches with <em>holes</em> and in the holes there are other smaller patches;</li>
<li>the area of these patches is too small.</li>
</ol>
<p><code>Thaumato Anakalyptor</code> tackles this issue with <em>stitching</em>.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="stitching">Stitching<a href="#stitching" class="hash-link" aria-label="Direct link to Stitching" title="Direct link to Stitching">​</a></h4>
<p>Stitching is an extension of the point cloud segmentation problem addressed in the previous step, but on patches rather than points.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[250px]" src="/img/tutorials/stitching.webp"></div><figcaption class="mt-1">Figure 5. Stitching adjacent patches (with the same winding number) together.</figcaption></figure>
<p>In Figure 5 different patches have different colors and patches that are stitched together are linked with red arcs. In the end, all the patches that are stitched together will form a segment, hopefully big enough to read on it some columns of text with ink detection models. It is worth saying that the segmentation performed by <code>Thaumato Anakalyptor</code> generates <em>overlapping</em> patches. The patch overlap is exploited during the stitching process.
<code>Thaumato Anakalyptor</code> performs stitching in a sort of Monte Carlo fashion. It first builds <em>an uncertainty graph</em> where nodes of this graph are patches and edges are weighted by the amount of overlap between patches. Many random walks are launched of the graph to build a subgraph (to select a cover of nodes/patches). The covers that selected the edges with the maximal overlap are chosen as final segments.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="mesh-reconstruction">Mesh reconstruction<a href="#mesh-reconstruction" class="hash-link" aria-label="Direct link to Mesh reconstruction" title="Direct link to Mesh reconstruction">​</a></h4>
<p>Now that we have a few big patches, represented as groups of point clouds made of contiguous points on the surface of the sheet, we can select one, and perform <em>mesh reconstruction</em>.</p>
<p>This means finally to establish a connectivity relationship between points: to decide which nodes that are connected in the fisherman net are connected in the point cloud as well. In layman&#x27;s term, we find a way to connect points with edges!</p>
<p>Surface reconstruction can be performed with several algorithms, such as <code>Poisson Surface Reconstruction</code> <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> (that leverages both the positions and the normals to the points, i.e. the gradients that we computed in the previous steps) and <code>Delaunay&#x27;s triangulation</code> <sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>. <code>Thaumato Anakalyptor</code> uses a mixture of both.</p>
<p>If instead of working on subvolumes we work on the full volume of the scroll, we can envisage ending up with a large contiguous mesh that is wrapped several times around the umbilicus. This is what <code>Thaumato Anakalyptor</code> can do, visualized in Figure 6.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[250px] m-2" src="/img/tutorials/mesh_0.webp"><img class="max-h-[250px] m-2" src="/img/tutorials/mesh_1.webp"><img class="max-h-[250px] m-2" src="/img/tutorials/mesh_2.webp"></div><figcaption class="mt-1">Figure 6. Large mesh wrapped several times around the center. (Left) Side view; (center) from above; (right) zoomed-in view.</figcaption></figure>
<p>In Figure 6, the vertices of the mesh are shown in yellow and the edges connecting them in black. It is worth noting from the zoomed-in view that nearby points are connected by edges and form a triangular mesh.</p>
<p>Congratulations! You automatically fished the scroll, or a subpart of it!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="further-steps">Further steps<a href="#further-steps" class="hash-link" aria-label="Direct link to Further steps" title="Direct link to Further steps">​</a></h3>
<p>In this extra section we describe how from a triangular mesh we can obtain the 2D image of a sheet. This process involves two main steps: <em>flattening</em> and <em>rendering</em>.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="flattening">Flattening<a href="#flattening" class="hash-link" aria-label="Direct link to Flattening" title="Direct link to Flattening">​</a></h4>
<p>Now that we have a mesh, we must compute a 3D-&gt;2D map to obtain a UV (two-dimensional) parametrization for its vertices. This is a fancy way to say that you have to flatten the mesh. After all, our purpose is to eventually read the ink on a <em>flat</em> image.</p>
<p>A community fork <sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>, recently merged to <code>Thaumato Anakalyptor</code>, implements an algorithm called <code>SLIM</code> <sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup> that aims to find a map that minimizes an <em>isometric distortion energy</em>. This means that all the points that are equidistant in 3D will be mapped as equidistant as possible in 2D.
The vertices of the mesh shown in Figure 6 are displayed using the obtained UV parametrization in Figure 7.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[500px]" src="/img/tutorials/flat_mesh.webp"></div><figcaption class="mt-1">Figure 7. UV parametrization of the vertices of the mesh displayed in Figure 6.</figcaption></figure>
<p>The mesh, along with its UV parametrization, are finally saved in a <code>.obj</code> file.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="rendering">Rendering<a href="#rendering" class="hash-link" aria-label="Direct link to Rendering" title="Direct link to Rendering">​</a></h4>
<p>It is extremely important that the obtained UV coordinates are still real numbers. This means that between points there is a lot of empty space!
In order to go back to a &quot;pixel-style&quot; representation, we have to convert the plot in Figure 7 to a pixelized image. Notice that in Figure 7 we displayed axes with ticks, this was not a mistake.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="per-pixel-map-and-layers">Per-pixel-map and layers<a href="#per-pixel-map-and-layers" class="hash-link" aria-label="Direct link to Per-pixel-map and layers" title="Direct link to Per-pixel-map and layers">​</a></h5>
<p>For every integer couple (x,y) in the figure we insert a new point. These coordinates (x,y) will be the integers&#x27; positions (i,j) of pixels in the rendered image. Eventually, we will end up with a number of points way higher than that of the vertices of the mesh. We will identify in which triangle of the triangular mesh (in 2D) these points fall and compute their <code>barycentric coordinates</code>: three coefficients that allow to define the position of every point in a triangle as a linear combination of the positions of the vertices. In Figure 8 we show a red point whose position will be <code>0.2 * vertex1 + 0.3 vertex2 + 0.5 vertex3</code>.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[400px] m-2" src="/img/tutorials/barycentric.webp"></div><figcaption class="mt-1">Figure 8. A red point in a triangle represented with its barycentric coordinates.</figcaption></figure>
<p>Exploiting the barycentric coordinates, we can then map back every additional point that we inserted in the UV map to its alleged position in 3D space.
This step is called obtaining a <em>per-pixel-map</em> and it&#x27;s performed in <code>Thaumato Anakalyptor</code> using a recently merged community fork <sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>.</p>
<p>Why do we need the 3D positions of the new points? Remember that during the first step, <em>Node Placement</em>, we totally forgot about the density of the material obtained via the CT scan (its original color) and started working on a black and white mask, that later became a point cloud, and so on. All the colors displayed in Figures 3-7 were only for representation purposes.</p>
<p>Now we need to recover that information, and to do so we need to know the 3D position of every new point that is going to become a pixel of our rendered image.
Unfortunately, the map from integer 2D positions to 3D points will result in points with non-integer positions. What does it mean? After all, the original voxels (Figure 1) <em>only</em> have integer positions. What &quot;color&quot; will these new points/pixels then have?</p>
<p>We are going to compute the color of the new points by <code>trilinear interpolation</code> with their closest ones in the volume.
This will allow us to obtain images with a smooth varying color. The segment from Figure 7 rendered as a 2D image is displayed in Figure 9.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[500px] m-2" src="/img/tutorials/thaumato_composite_down.webp"></div><figcaption class="mt-1">Figure 9. The segment from Figure 7 rendered as a 2D image.</figcaption></figure>
<p>We forget to mention that since ink prediction is performed not on a single layer, but in what is called a surface volume, that is a stack of surfaces parallel to the surface we obtained along this pipeline, we need to store and transform as well the values of the normals to the surface at each point.
We already computed them for the vertices of the mesh, so we only need to compute the ones for the new points using barycentric coordinates.</p>
<p>In Figure 10 we show the ink prediction from the <code>Phase 1 Grand Prize winning model</code> <sup><a href="#user-content-fn-8" id="user-content-fnref-8" data-footnote-ref="true" aria-describedby="footnote-label">8</a></sup> on the stack of rendered layers obtained from the segment displayed in Figure 7 - the composite image is shown in Figure 9.</p>
<figure class="text-center mx-auto"><div class="flex flex-wrap justify-center mx-auto"><img class="max-h-[500px] m-2" src="/img/tutorials/thaumato_ink_detection.webp"></div><figcaption class="mt-1">Figure 10. Ink prediction on the layers rendered from the UV in Figure 7.</figcaption></figure>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="additional-notes">Additional notes<a href="#additional-notes" class="hash-link" aria-label="Direct link to Additional notes" title="Direct link to Additional notes">​</a></h3>
<p>Please notice that every single one of the previously mentioned steps can be improved, both in terms of mesh generation quality and computational efficiency.
For further information on <code>Thaumato Anakalyptor</code> and its roadmap, please read its official <a href="https://github.com/schillij95/ThaumatoAnakalyptor/blob/main/documentation/ThaumatoAnakalyptor___Technical_Report_and_Roadmap.pdf" target="_blank" rel="noopener noreferrer">Technical Report</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h3>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1">
<p>Schilliger et al. (2023). <em>Thaumato Anakalyptor</em>. Retrieved from <a href="https://github.com/schillij95/ThaumatoAnakalyptor" target="_blank" rel="noopener noreferrer">https://github.com/schillij95/ThaumatoAnakalyptor</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Schult et al. (2023). <em>Mask3D: Mask Transformer for 3D Semantic Instance Segmentation</em>. Paper presented at the International Conference on Robotics and Automation (ICRA). <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Kazhdan et al. (2006). <em>Poisson Surface Reconstruction</em>. In Proceedings of the Eurographics Symposium on Geometry Processing. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>Delaunay, B. (1934). <em>Sur la sphère vide</em>. Bulletin de l&#x27;Académie des Sciences de l&#x27;URSS, Classe des Sciences Mathématiques et Naturelles, 6, 793-800. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-5">
<p>Angelotti, G. (2024). <em>slim-flatboi</em>. Retrieved from <a href="https://github.com/giorgioangel/slim-flatboi" target="_blank" rel="noopener noreferrer">https://github.com/giorgioangel/slim-flatboi</a> and <a href="https://github.com/schillij95/ThaumatoAnakalyptor/pull/2" target="_blank" rel="noopener noreferrer">https://github.com/schillij95/ThaumatoAnakalyptor/pull/2</a> <a href="#user-content-fnref-5" data-footnote-backref="" aria-label="Back to reference 5" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-6">
<p>Rabinovich et al. (2017). <em>Scalable Locally Injective Mappings</em>. ACM Transactions on Graphics (TOG), 36(4), 1. <a href="#user-content-fnref-6" data-footnote-backref="" aria-label="Back to reference 6" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>Angelotti, G. (2024). <em>Democratizing rendering</em>. Retrieved from <a href="https://github.com/schillij95/ThaumatoAnakalyptor/pull/6" target="_blank" rel="noopener noreferrer">https://github.com/schillij95/ThaumatoAnakalyptor/pull/6</a> <a href="#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 7" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-8">
<p>Nader et al. (2023). <em>Vesuvius Grandprize Winning Solution</em>. Retrieved from <a href="https://github.com/younader/Vesuvius-Grandprize-Winner" target="_blank" rel="noopener noreferrer">https://github.com/younader/Vesuvius-Grandprize-Winner</a> <a href="#user-content-fnref-8" data-footnote-backref="" aria-label="Back to reference 8" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/ScrollPrize/villa/tree/main/scrollprize.org/docs/06_tutorial_thaumato.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Overview</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/deepast/get_started">Getting Started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Deep Past Challenge.</div></div></div></footer></div>
</body>
</html>